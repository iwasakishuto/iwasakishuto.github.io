<!doctype html>
<html lang="ja">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../static/css/atom.min.css">
    <!--
      Hi source code lover!!

      I don't want to be a YouTuber.
      I want to make a platform where people can share and learn college knowledge one another.
      If you are interested it, please get in touch with me. (Twitter: @cabernet_rock)
    -->

    <!-- SEO -->
    <title>10 minutes PRML Chapter 13</title>
    <meta name="description" content="Learn Pattern Recognition and Machine Learning in 10 minutes.">

    <!-- URL CANONICAL -->
    <!-- <link rel="canonical" href="http://your-url.com/permalink"> -->

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,700,700i%7CMaitree:200,300,400,600,700&amp;subset=latin-ext" rel="stylesheet">

    <!-- CSS Base -->
    <link rel="stylesheet" type='text/css' media='all' href="prml_static/css/webslides.css">

    <!-- Optional - CSS SVG Icons (Font Awesome) -->
    <link rel="stylesheet" type="text/css" media="all" href="prml_static/css/svg-icons.css">

    <!-- SOCIAL CARDS (Open Graph protocol) -->
    <!-- FACEBOOK -->
    <meta property="og:url" content="https://iwasakishuto.github.io">
    <meta property="og:type" content="article">
    <meta property="og:title" content="10 minutes PRML Chapter 13">
    <meta property="og:description" content="Learn Pattern Recognition and Machine Learning in 10 minutes.">
    <meta property="og:image" content="prml_static/images/share-webslides.jpg" >

    <!-- TWITTER -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:creator" content="@cabernet_rock">
    <meta name="twitter:title" content="10 minutes Chapter 13">
    <meta name="twitter:description" content="Learn Pattern Recognition and Machine Learning in 10 minutes.">
    <meta name="twitter:image" content="prml_static/images/share-webslides.jpg">

    <!-- FAVICONS -->
    <link rel="shortcut icon" sizes="16x16" href="prml_static/images/favicons/favicon.png">
    <link rel="shortcut icon" sizes="32x32" href="prml_static/images/favicons/favicon-32.png">
    <link rel="apple-touch-icon icon" sizes="76x76" href="prml_static/images/favicons/favicon-76.png">
    <link rel="apple-touch-icon icon" sizes="120x120" href="prml_static/images/favicons/favicon-120.png">
    <link rel="apple-touch-icon icon" sizes="152x152" href="prml_static/images/favicons/favicon-152.png">
    <link rel="apple-touch-icon icon" sizes="180x180" href="prml_static/images/favicons/favicon-180.png">
    <link rel="apple-touch-icon icon" sizes="192x192" href="prml_static/images/favicons/favicon-192.png">

    <!-- Android -->
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="theme-color" content="#333333">

    <!-- Syntax highlight -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!-- Tex -->
    <!-- Local env -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- Github env -->
    <!--
    <script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
      });
    </script>
  </head>

  <body>
    <header role="banner">
      <nav role="navigation">
        <ul>
          <li class="github">
            <a rel="external" href="#" title="YouTube">
              <svg class="fa-youtube">
                <use xlink:href="#fa-youtube"></use>
              </svg>
              <em>Colledge Knowledge</em>
            </a>
          </li>
          <li class="twitter">
            <a rel="external" href="https://twitter.com/cabernet_rock" title="Twitter">
              <svg class="fa-twitter">
                <use xlink:href="#fa-twitter"></use>
              </svg>
              <em>@cabernet_rock</em>
            </a>
          </li>
        </ul>
      </nav>
    </header>

    <main role="main">
      <article id="webslides">

        <!-- Quick Guide
          - Each parent <section> in the <article id="webslides"> element is an individual slide.
          - Vertical sliding = <article id="webslides" class="vertical">
          - <div class="wrap"> = container 90% / <div class="wrap size-50"> = 45%;
        -->

        <section class="bg-apple">
          <h1>§13 Sequential Data</h1>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>系列データ</h2>
                <p class="text-intro">Sequential Data</p>
                <p>ここまでは、<font color="red"><b>独立同分布(independent identically distributed)</b></font>に従うと仮定するデータ集合について主に議論してきました。この仮定の下では、各々のデータ点における確率分布の全てのデータ点にわたる積として尤度関数を表すことができました。</p>
                <p>しかし、この独立同分布の仮定が当てはまらない応用は数多くあります。そこで、ここではそのようなデータ集合の中でもとりわけ重要なクラスである、系列データについて考察します。系列データは、主に時系列の測定を通して得られる場合が多く、外国為替レートの値や<font color="red"><b>音声認識(speech recognition)</b></font>に用いられる連続時間フレームにおける音響特徴量などが、例として挙げられます。</p>
                <p>便宜上、系列における「過去」や「未来」の観測データという用語を用いますが、ここで検討するモデルは、時系列だけでなく、全ての系列データに対しても用いることができます。</p>
              </div>
              <div class="column">
                <p>系列データを扱う上で、「定常」と「非定常」な系列の分布を分けて考えることは有益です。定常な場合、データは時間とともに変化していきますが、<font color="red"><b>データを生成する分布は常に同じ</b></font>です。一方非定常な状況では、<font color="red"><b>生成分布それ自体が時間とともに変化</b></font>します。ここでは、定常な場合に焦点を当てます。</p>
                <p>系列データを扱うときに重要となるヒューリスティックは、<font color="red"><b>将来の値を予測するには、直近の観測の方がそれより過去の観測よりも有益である</b></font>と期待することです。</p>
                <p>また、<font color="red"><b>未来の予測値が過去のすべての観測値に広く依存している</b></font>と考えることは、現実的ではありません。そこで、<font color="red"><b>未来の予測値が、直近の観測値以外の過去の観測値に対して独立である</b></font>という仮定をもつ<font color="red"><b>マルコフモデル</b></font>について考察します。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>マルコフモデル</h2>
                <p class="text-intro">Markov model</p>
                <p>系列データを扱う最も簡単な方法は、系列であるという性質を無視して、観測値が独立同分布に従うとして扱うことです。</p>
                <p>しかし、この方法は系列の中で近い位置にある観測値間の送還などの、データの順序に関係するパターンを捉えることができません。</p>
                <p>そこで、これらの性質を表現するために、独立同分布の仮定を緩めます。まず、一般性を失うことなく、確率の積の規則を用いて、観測系列の同時分布を以下の形で表現することができます。
                $$p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\right)=\prod_{n=1}^{N} p\left(\mathbf{x}_{n} | \mathbf{x}_{1}, \ldots, \mathbf{x}_{n-1}\right)\qquad (13.1)$$</p>
              </div>
              <div class="column">
                <p>ここで、右辺の条件付き分布の各々が最も近い観測値以外の全ての過去の観測値から独立であると仮定すると、<font color="red"><b>一次マルコフ連鎖(first-order Markov chain)</b></font>を得ます。</p>
                <p>このモデルの下で、$N$ 個の観測系列の同時分布は以下のように与えられます。
                $$p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\right)=p\left(\mathbf{x}_{1}\right) \prod_{n=2}^{N} p\left(\mathbf{x}_{n} | \mathbf{x}_{n-1}\right)\qquad (13.2)$$</p>
                <p>ここで、有効分離の性質より、時刻 $n$ までの全ての観測値を与えた時の観測値 $\mathbf{x}_n$ の条件付き確率は以下で与えられます。
                $$p\left(\mathbf{x}_{n} | \mathbf{x}_{1}, \ldots, \mathbf{x}_{n-1}\right)=p\left(\mathbf{x}_{n} | \mathbf{x}_{n-1}\right)\qquad (13.3)$$</p>
                <p>すなわち、予測値の分布は、その直前の観測値のみに依存し、全てのそれ以前の観測から独立になります。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>マルコフモデル</h2>
                <p class="text-intro">Markov model</p>
                <p>これらのモデルの応用では、ほとんどの場合、モデルを定義する<font color="red"><b>条件付き分布 $p(\mathbf{x}_n|\mathbf{x}_{n+1})$ がみな同一である</b></font>という制約を課します。これは、定常時系列の仮定に対応します。このモデルは<font color="red"><b>均一マルコフ連鎖(homogeneous Markov model)</b></font>として知られます。なお、<font color="red"><b>斉次マルコフモデル</b></font>という訳し方もあるようです。</p>
                <p>このモデルを拡張して、より離れたデータ点が関係を持つモデルを表したいとします。これは、高次のマルコフ連鎖へ移行することで達成されます。</p>
                <p>もし、予測値が直前のさらに一つ前の値にも依存することを許すと、二次マルコフ連鎖を得ます。この場合、同時確率は以下の式で与えられます。
                $$p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\right)=p\left(\mathbf{x}_{1}\right) p\left(\mathbf{x}_{2} | \mathbf{x}_{1}\right) \prod_{n=3}^{N} p\left(\mathbf{x}_{n} | \mathbf{x}_{n-1}, \mathbf{x}_{n-2}\right)\qquad (13.4)$$</p>
              </div>
              <div class="column">
                <p>同様にして、$M$ 次のマルコフ連鎖を考えれば、予測値が直前 $M$ 個のデータに依存するモデルを考えることができます。</p>
                <p>しかし、このように柔軟性を増しすぎると、代償としてモデルのパラメータ数が急激に増加します。</p>
                <p>もし変数が離散変数で、条件付き分布が共通の条件付き確率表で表されるとすると、そのモデルのパラメータ数は $K^M(K-1)$ となり、$M$ の値に関して指数的に増大してしまいます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>状態空間モデル</h2>
                <p class="text-intro">state space model</p>
                <p>先の問題に対処しつつ、離れたデータ点の関係性を表現するモデルを作ることを考えます。</p>
                <p>これは、潜在変数を導入し、単純な成分から豊かな表現力を持つモデルのクラスを作ることで可能になります。</p>
                <p>今、各々の観測値 $\mathbf{x}_n$ に対し、対応する潜在変数 $\mathbf{z}_n$ を導入します。ここで、<font color="red"><b>マルコフ連鎖を構成するのが潜在変数である</b></font>と仮定することにより、次のような<font color="red"><b>状態空間モデル(state space model)</b></font>と呼ばれるグラフ構造を得ることができます。</p>
              </div>
              <div class="column">
                <figure>
                  <img src="prml_static/images/Chap13/hidden Markov model.png" alt="hidden Markov model">
                </figure>
                <p>これは、$\mathbf{z}_n$ を与えられた時、$\mathbf{z}_{n-1}$ と $\mathbf{z}_{n+1}$ が独立であるという重要な条件付き独立性を満たします。</p>
                <p>従って、このモデルの同時分布は以下で与えられます。
                $$p\left(\mathrm{x}_{1}, \ldots, \mathrm{x}_{N}, \mathrm{z}_{1}, \ldots, \mathrm{z}_{N}\right)=p\left(\mathrm{z}_{1}\right)\left[\prod_{n=2}^{N} p\left(\mathrm{z}_{n} | \mathrm{z}_{n-1}\right)\right] \prod_{n=1}^{N} p\left(\mathrm{x}_{n} | \mathrm{z}_{n}\right)\qquad (13.6)$$</p>
                <p>この時、潜在変数を経由して二つの観測変数 $\mathbf{x}_{n}$ と $\mathbf{x}_{m}$ をつなぐ経路は常に存在し、この経路は決して遮断されないことがわかります。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>状態空間モデル</h2>
                <p class="text-intro">state space model</p>
                <p>したがって、過去の観測値すべてを与えたときの、観測 $\mathbf{x}_{n+1}$ の予測分布 $p(\mathbf{x}_{n+1}|\mathbf{x}_1,\ldots,\mathbf{x}_n)$ は、条件付き独立の性質を何も持たず、これより $\mathbf{x}_{n+1}$ の予測は全ての過去の観測値に依存します。</p>
                <p>その一方で、観測変数はどの次数のマルコフ性も満たしません。</p>
                <p>このグラフによって表現される系列データのモデルとして、次の二つのモデルが重要となります。</p>
              </div>
              <div class="column">
                <p class="text-intro">隠れマルコフモデル(HMM; hidden Markov model)</p>
                <p>HMMの観測変数は離散変数でも連続変数でもよく、様々な条件付き分布をそのモデル化に用いることができます。なお、一般に隠れ変数がカテゴリカル変数であるものを指します。</p>
                <p class="text-intro">線形動的システム(linear dynamical system)</p>
                <p>潜在変数と観測変数の両方がガウス分布に従う場合（そして、それらの親に対する条件付き分布が線形ガウス分布であるとき）のモデル化に用いることができます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>隠れマルコフモデル</h2>
                <p class="text-intro">hidden Markov model</p>
                <p>隠れマルコフモデルに従う観測データ $\{\mathbf{x}_n\}$ と隠れ変数 $\{\mathbf{z}_n\}$ の同時分布は以下のようになります。
                $$p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta})=p\left(\mathbf{z}_{1} | \boldsymbol{\pi}\right)\left[\prod_{n=2}^{N} p\left(\mathbf{z}_{n} | \mathbf{z}_{n-1}, \mathbf{A}\right)\right] \prod_{m=1}^{N} p\left(\mathbf{x}_{m} | \mathbf{z}_{m}, \boldsymbol{\phi}\right)\qquad (13.10)$$</p>
                <p>したがって、均一マルコフ連鎖(斉次モデル)であれば、以下の三種類のパラメータ $\{\boldsymbol{\pi},\mathbf{A},\boldsymbol{\phi}\} = \boldsymbol{\theta}$ が必要となります。
                  <li>初期状態(initial state): $p(\mathbf{z}_1)$ を記述するパラメータ $\boldsymbol{\pi}$</li>
                  <li>遷移確率(transition probability): $p(\mathbf{z}_n|\mathbf{z}_{n-1})$ を記述するパラメータ $\mathbf{A}$</li>
                  <li>出力確率(emission probability): $p(\mathbf{x}_n|\mathbf{z}_n)$ を記述するパラメータ $\boldsymbol{\phi}$</li>
                </p>
              </div>
              <div class="column">
                <p>最初の潜在ノード $\mathbf{z}_1$ は、要素 $\pi_k\equiv p(z_{1 k} = 1)$ をもつ確率のベクトル $\boldsymbol{\pi}$ で表される周辺分布 $p(\mathbf{z}_1)$ を持ちます。
                $$p\left(\mathbf{z}_{1} | \boldsymbol{\pi}\right)=\prod_{k=1}^{K} \pi_{k}^{z_{1 k}}\qquad (13.8)$$</p>
                <p>遷移確率は、$A_{j k}\equiv p(z_{n k} = 1|z_{n-1,j} = 1)$ で定義されるので、以下のように書けます。
                $$p\left(\mathbf{z}_{n} | \mathbf{z}_{n-1, \mathbf{A}}\right)=\prod_{k=1}^{K} \prod_{j=1}^{K} A_{j k}^{z_{n-1, j} z_{n k}}\qquad (13.7)$$</p>
                <p>放出確率はどのような分布も考えることができます。$z_n = k$ の場合の分布のパラメータを $\phi_k$ とすれば、以下の形で書くことができます。
                $$p\left(\mathbf{x}_{n} | \mathbf{z}_{n}, \boldsymbol{\phi}\right)=\prod_{k=1}^{K} p\left(\mathbf{x}_{n} | \boldsymbol{\phi}_{k}\right)^{z_{n k}}\qquad (13.9)$$</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>HMMの学習</h2>
                <p>それでは、HMMの学習の流れを見ていきます。データ集合 $\mathbf{X} = \{\mathbf{x}_1,\ldots,\mathbf{x}_N\}$ が観測された場合、HMMのパラメータを最尤推定で決定することができます。</p>
                <p>同時分布の式
                $$p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta})=p\left(\mathbf{z}_{1} | \boldsymbol{\pi}\right)\left[\prod_{n=2}^{N} p\left(\mathbf{z}_{n} | \mathbf{z}_{n-1}, \mathbf{A}\right)\right] \prod_{m=1}^{N} p\left(\mathbf{x}_{m} | \mathbf{z}_{m}, \boldsymbol{\phi}\right)\qquad (13.10)$$</p>
                を潜在変数について周辺化することで、以下の尤度関数が得られます。
                $$p(\mathbf{X} | \boldsymbol{\theta})=\sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta})\qquad (13.11)$$</p>
                <p>ここで、同時分布 $p(\mathbf{X} | \boldsymbol{\theta})$ は $n$ について分解することができないので、各々の $\mathbf{z}_n$ に関する和を独立なものとして単純に扱うことができません。</p>
              </div>
              <div class="column">
                <p>さらに、和を取る対象が $N$ 個の変数で、その各々が $K$ 状態を取る結果、全部で $K^N$ 項になり、明示的に和の演算を実行することもできません。</p>
                <p>また、尤度関数(13.11)におけるさらなる困難は、混合分布の一般化に相当しているために、潜在変数の異なる値に対する出力モデルの和になっている点です。そのため、尤度関数を直接最大化しようとすると、閉じた形の解を持たない複雑な式になってしまいます。</p>
                <p>そこで、ここでは、隠れマルコフモデルの尤度関数を最大化する効率的な枠組みとして、<font color="red"><b>EM(expectation maximization)</b></font>アルゴリズムを考えることにします。</p>
                <p>また、変分ベイズやランダムサンプリングを用いた近似法も利用します。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>EMアルゴリズム</h2>
                <p class="text-intro">E step</p>
                <p>Eステップでは、パラメータ $\boldsymbol{\theta}$ の値を利用して、潜在変数 $p(\mathbf{Z}|\mathbf{X},\boldsymbol{\theta}^{\mathrm{old}})$ の事後分布を求めます。</p>
                <p>次に、この事後分布を用いて、パラメータ集合 $\boldsymbol{\theta}$ の関数としての、<font color="red"><b>完全データに対する尤度関数の対数の期待値</b></font>を求めます。この期待値は、以下の関数 $Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text { old }}\right)$ で定義されます。
                $$Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text { old }}\right)=\sum_{\mathbf{Z}} p(\mathbf{Z} | \mathbf{X}, \boldsymbol{\theta}^{\text { old }}) \ln p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta})\qquad (13.12)$$</p>
                <p>ここで、表記を簡単にするため、いくつかの記号を導入します。</p>
              </div>
              <div class="column">
                <p>
                  <li>潜在変数 $\mathbf{z}_n$ の周辺事後分布 $\gamma$:
                  $$\begin{aligned}
                  \gamma\left(\mathbf{z}_{n}\right)
                  & =p\left(\mathbf{z}_{n} | \mathbf{X}, \boldsymbol{\theta}^{\text { old }}\right)
                  & (13.13)\\
                  \gamma\left(z_{n k}\right)
                  & =\mathbb{E}\left[z_{n k}\right]=\sum_{\mathbf{z}} \gamma(\mathbf{z}) z_{n k}
                  & (13.15)
                  \end{aligned}$$
                  <li>二つの連続した潜在変数に対する同時事後分布 $\xi$:
                  $$\begin{aligned}
                  \xi\left(\mathbf{z}_{n-1}, \mathbf{z}_{n}\right)
                  & =p\left(\mathbf{z}_{n-1}, \mathbf{z}_{n} | \mathbf{X}, \boldsymbol{\theta}^{\text { old }}\right)
                  & (13.14)\\
                  \xi\left(z_{n-1, j}, z_{n k}\right)
                  & =\mathbb{E}\left[z_{n-1, j} z_{n k}\right]=\sum_{\mathbf{z}} \gamma(\mathbf{z}) z_{n-1, j} z_{n k}
                  & (13.16)
                  \end{aligned}$$</li>
                </p>
                <p>これらを用いることで、Eステップで最大化したい $Q$ 関数が以下のように表されます。
                $$\begin{aligned}
                Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text { old }}\right)
                =& \sum_{k=1}^{K} \gamma\left(z_{1 k}\right) \ln \pi_{k}+\sum_{n=2}^{N} \sum_{j=1}^{K} \sum_{k=1}^{K} \xi\left(z_{n-1, j}, z_{n k}\right) \ln A_{j k} \\
                & +\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right) \ln p\left(\mathbf{x}_{n} | \boldsymbol{\phi}_{k}\right)\qquad (13.17)
                \end{aligned}$$
                </p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>EMアルゴリズム</h2>
                <p class="text-intro">M step</p>
                <p>Mステップでは、$\gamma(\mathbf{z}),\xi(\mathbf{z}_{n-1},\mathbf{z})$ を定数とみなし、パラメータ $\boldsymbol{\theta} = \{\boldsymbol{\pi},\mathbf{A},\boldsymbol{\phi}\}$ に関して $Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text { old }}\right)$ を最大化します。</p>
                <p>ただし、$\boldsymbol{\pi},\mathbf{A}$ には制約条件が付くので、ラグランジュの未定乗数を導入して、以下の式を解く必要があります。
                $$\begin{aligned}
                &Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text { old }}\right)
                =\sum_{k=1}^{K} \gamma\left(z_{1 k}\right) \ln \pi_{k}+\sum_{n=2}^{N} \sum_{j=1}^{K} \sum_{k=1}^{K} \xi\left(z_{n-1, j}, z_{n k}\right) \ln A_{j k} \\
                & +\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right) \ln p\left(\mathbf{x}_{n} | \boldsymbol{\phi}_{k}\right) + \lambda_1\left(\sum_{k=1}^K\pi_k - 1\right) + \sum_{j=1}^K\lambda_{2,j}\left(\sum_{k=1}^K A_{jk} - 1\right)
                \end{aligned}$$</p>
                <p>難しく見えますが、$\boldsymbol{\pi},\mathbf{A},\boldsymbol{\phi}$ を含む項は別々になっているので、計算は楽です。</p>
              </div>
              <div class="column">
                <p>$\boldsymbol{\pi}$ に関する最大化</p>
                <p>以下を $\boldsymbol{\pi}$ に関して最大化すれば良いです。
                $$Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text { old }}\right) = \sum_{k=1}^{K} \gamma\left(z_{1 k}\right) \ln \pi_{k} + \lambda_1\left(\sum_{k=1}^K\pi_k - 1\right)$$</p>
                <p>したがってこれは簡単に求まり、
                $$\pi_{k}=\frac{\gamma\left(z_{1 k}\right)}{\sum_{j=1}^{K} \gamma\left(z_{1 j}\right)}\qquad (13.20)$$
                となります。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <p class="text-intro">$\mathbf{A}$ に関する最大化</p>
                <p>同様に、以下を $\mathbf{A}$ に関して最大化すれば良いです。
                $$\begin{aligned}
                Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text { old }}\right)
                & = \sum_{n=2}^{N} \sum_{j=1}^{K} \sum_{k=1}^{K} \xi\left(z_{n-1, j}, z_{n k}\right) \ln A_{j k}\\
                & + \sum_{j=1}^K\lambda_{2,j}\left(\sum_{k=1}^K A_{jk} - 1\right)
                \end{aligned}$$</p>
                <p>これも簡単に求まり、以下を得ます。
                $$A_{j k}=\frac{\sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n k}\right)}{\sum_{l=1}^{K} \sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n l}\right)}\qquad (13.19)$$</p>
              </div>
              <div class="column">
                <p class="text-intro">$\phi_k$ に関する最大化</p>
                <p>$Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text { old }}\right)$ を $\phi_k$ に関して最大化する場合、以下を $\phi_k$ に関して最大化すれば良いことがわかります。
                $$Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text { old }}\right) = \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right) \ln p\left(\mathbf{x}_{n} | \boldsymbol{\phi}_{k}\right)$$</p>
                <p>これは、$\gamma(z_{n k})$ が負担率の役割を果たす標準的な混合分布に対する $Q$ 関数のデータ依存項と全く同一です。したがって、重み $\gamma_{n k}$ で重み付けられた出力確率 $p(\mathbf{x}|\phi_k)$ の重み付き対数尤度関数を単に最大化すれば良くなります。</p>
                <p>例えば出力分布がガウス密度関数の時($p(\mathbf{x}|\phi_k) = \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)$)には以下を得ます。
                $$\begin{aligned}
                \boldsymbol{\mu}_{k}
                & =\frac{\sum_{n=1}^{N} \gamma\left(z_{n k}\right) \mathbf{x}_{n}}{\sum_{n=1}^{N} \gamma\left(z_{n k}\right)}
                & (13.20)\\
                \Sigma_{k}
                & =\frac{\sum_{n=1}^{N} \gamma\left(z_{n k}\right)\left(\mathrm{x}_{n}-\mu_{k}\right)\left(\mathrm{x}_{n}-\mu_{k}\right)^{\mathrm{T}}}{\sum_{n=1}^{N} \gamma\left(z_{n k}\right)}
                & (13.21)
                \end{aligned}$$
                </p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>Baum-Welch</h2>
                <p class="text-intro">フォワード-バッグワードアルゴリズム</p>
                <p>それでは、EMアルゴリズムのEステップで $\gamma(z_{n k}),\xi(z_{n-1,j},z_{n k})$ を効率的に求めるアルゴリズムを検討します。</p>
                <p>以下の図のように、HMMのグラフは木構造をもちます。したがって、二段階のメッセージパッシングアルゴリズムをによって潜在変数の事後確率 $\gamma_{n k}$ を効率良く求めることができます。これを、<font color="red"><b>フォワード-バッグワードアルゴリズム(forward-backward algorithm)</b></font>や<font color="red"><b>Baum-Welchアルゴリズム</b></font>と呼びます。</p>
                <figure>
                  <img src="prml_static/images/Chap13/hidden Markov model.png" alt="hidden Markov model">
                </figure>
              </div>
              <div class="column">
                <p>まず、ベイズの定理より以下が成立するので、
                $$\gamma\left(\mathbf{z}_{n}\right)=p\left(\mathbf{z}_{n} | \mathbf{X}\right)=\frac{p(\mathbf{X} | \mathbf{z}_{n}) p\left(\mathbf{z}_{n}\right)}{p(\mathbf{X})}\qquad (13.32)$$
                $\boldsymbol{\theta}^{\mathrm{old}}$ を使って $p(\mathbf{x}|\mathbf{z}_n)$ を計算することができれば $\gamma_{n k}$ を求めることができます。</p>
                <p>ここで、$z_n$ の値が定まると $n$ より過去より未来が独立になるので、
                $$p(\mathbf{X} | \mathbf{z}_{n})= p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n} | \mathbf{z}_{n}\right)p\left(\mathbf{x}_{n+1}, \ldots, \mathbf{x}_{N} | \mathbf{z}_{n}\right)$$</p>
                <p>これと $p(\mathbf{x}_1,\ldots,\mathbf{x}_n|z_n)p(z_n) = p(\mathbf{x}_1,\ldots,\mathbf{x}_n,z_n)$ であることを利用すれば、
                $$\gamma_{n k} = p(\mathbf{z}_n|\mathbf{x}) = \frac{p(\mathbf{x}_1,\ldots,\mathbf{x}_n,\mathbf{z}_n)p(\mathbf{x}_{n+1},\ldots,\mathbf{x}_N|\mathbf{z}_n)}{p(\mathbf{x})}$$
                と表すことができます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>Baum-Welch</h2>
                <p class="text-intro">フォワード-バッグワードアルゴリズム</p>
                <p>そこで、
                $$\begin{aligned}
                \alpha(\mathbf{z}_n)
                & = p(\mathbf{x}_1,\ldots,\mathbf{x}_n,\mathbf{z}_n)
                & (13.34)\\
                \beta(z_i)
                & = p(\mathbf{x}_{n+1},\ldots,\mathbf{x}_N|\mathbf{z}_n)
                & (13.35)
                \end{aligned}$$
                と定義します。これらを用いれば、
                $$\gamma\left(\mathbf{z}_{n}\right)=\frac{p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}, \mathbf{z}_{n}\right) p\left(\mathbf{x}_{n+1}, \ldots, \mathbf{x}_{N} | \mathbf{z}_{n}\right)}{p(\mathbf{X})}
                =\frac{\alpha\left(\mathbf{z}_{n}\right) \beta\left(\mathbf{z}_{n}\right)}{p(\mathbf{X})}\qquad (13.33)$$
                のように表すことができます。</p>
                <p>ここで目的となるのは、$\alpha$ の漸化式を求めること(<font color="red"><b>前向き</b></font>)と $\beta$ の漸化式を求めること(<font color="red"><b>後ろ向き</b></font>)です。</p>
              </div>
              <div class="column">
                <p class="text-intro">前向き</p>
                <p>条件付き独立性や加法・乗法定理を用いることで、以下のように $\alpha(\mathbf{z}_{n-1})$ を用いて $\alpha(\mathbf{z}_n)$ を表すことが可能になります。
                $$\begin{aligned}
                \alpha\left(\mathbf{z}_{n}\right)
                & =p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}, \mathbf{z}_{n}\right) \\
                & =p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n} | \mathbf{z}_{n}\right) p\left(\mathbf{z}_{n}\right) \\
                & =p\left(\mathbf{x}_{n} | \mathbf{z}_{n}\right) p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n-1} | \mathbf{z}_{n}\right) p\left(\mathbf{z}_{n}\right) \\
                & =p\left(\mathbf{x}_{n} | \mathbf{z}_{n}\right) p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n-1}, \mathbf{z}_{n}\right) \\
                & =p\left(\mathbf{x}_{n} | \mathbf{z}_{n}\right) \sum_{\mathbf{z}_{n-1}} p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n-1}, \mathbf{z}_{n-1}, \mathbf{z}_{n}\right) \\
                & =p\left(\mathbf{x}_{n} | \mathbf{z}_{n}\right) \sum_{\mathbf{z}_{n-1}} p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n-1}, \mathbf{z}_{n} | \mathbf{z}_{n-1}\right) p\left(\mathbf{z}_{n-1}\right) \\
                & =p\left(\mathbf{x}_{n} | \mathbf{z}_{n}\right) \sum_{\mathbf{z}_{n-1}} p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n-1} | \mathbf{z}_{n-1}\right) p\left(\mathbf{z}_{n} | \mathbf{z}_{n-1}\right) p\left(\mathbf{z}_{n-1}\right) \\
                & =p\left(\mathbf{x}_{n} | \mathbf{z}_{n}\right) \sum_{\mathbf{z}_{n-1}} p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n-1}, \mathbf{z}_{n-1}\right) p\left(\mathbf{z}_{n} | \mathbf{z}_{n-1}\right) \\
                & =p\left(\mathbf{x}_{n} | \mathbf{z}_{n}\right) \sum_{\mathbf{z}_{n-1}} \alpha(\mathbf{z}_{n-1})p(\mathbf{z} | \mathbf{z}_{n-1})\\
                \end{aligned}$$</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <p class="text-intro">前向き</p>
                <p>再帰式は、以下のようになります。
                $$\alpha\left(\mathbf{z}_{n}\right)=p\left(\mathbf{x}_{n} | \mathbf{z}_{n}\right) \sum_{\mathbf{z}_{n-1}} \alpha\left(\mathbf{z}_{n-1}\right) p\left(\mathbf{z}_{n} | \mathbf{z}_{n-1}\right)\qquad (13.36)$$</p>
                <p>この再帰を開始するためには、以下で与えられる初期条件が必要になります。
                $$\alpha\left(\mathbf{z}_{1}\right)=p\left(\mathbf{x}_{1}, \mathbf{z}_{1}\right)=p\left(\mathbf{z}_{1}\right) p\left(\mathbf{x}_{1} | \mathbf{z}_{1}\right)=\prod_{k=1}^{K}\left\{\pi_{k} p\left(\mathbf{x}_{1} | \boldsymbol{\phi}_{k}\right)\right\}^{z_{1 k}}\qquad (13.37)$$</p>
                <p>再帰の各段階が $K\times K$ 行列による乗算を含むので、鎖全体に対してこれらの値を求める全体のコストは $\mathcal{O}(K^2N)$ になります。</p>
              </div>
              <div class="column">
                <figure>
                  <img class="aligncenter" src="prml_static/images/Chap13/forward algorithm.png" alt="forward algorithm" width=50%>
                </figure>
                <br>
                <br>
                <p>$(n-1)$ ステップの $\alpha(\mathbf{z}_{n-1})$ の要素 $\alpha(z_{n-1,j})$ について、$p(\mathbf{z}_n|\mathbf{z}_{n-1})$ の値に相当する $A_{j1}$ で与えられる重みを用いて、重み付け和を取り、さらにそれにデータの寄与 $p(\mathbf{x}_n|z_{n1})$ を掛けることで $\alpha(z_{n,1})$ を得ています。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <p class="text-intro">後ろ向き</p>
                <p>同様にして、$\beta(\mathbf{z}_n)$ に関する再帰式は以下のように定義できます。
                $$\begin{aligned}
                \beta\left(\mathbf{z}_{n}\right)
                & =p\left(\mathbf{x}_{n+1}, \ldots, \mathbf{x}_{N} | \mathbf{z}_{n}\right) \\
                & =\sum_{\mathbf{z}_{n+1}} p\left(\mathbf{x}_{n+1}, \ldots, \mathbf{x}_{N}, \mathbf{z}_{n+1} | \mathbf{z}_{n}\right) \\
                & =\sum_{\mathbf{z}_{n+1}} p\left(\mathbf{x}_{n+1}, \ldots, \mathbf{x}_{N} | \mathbf{z}_{n}, \mathbf{z}_{n+1}\right) p\left(\mathbf{z}_{n+1} | \mathbf{z}_{n}\right) \\
                & =\sum_{\mathbf{z}_{n+1}} p\left(\mathbf{x}_{n+1}, \ldots, \mathbf{x}_{N} | \mathbf{z}_{n+1}\right) p\left(\mathbf{z}_{n+1} | \mathbf{z}_{n}\right) \\
                & =\sum_{\mathbf{z}_{n+1}} p\left(\mathbf{x}_{n+2}, \ldots, \mathbf{x}_{N} | \mathbf{z}_{n+1}\right) p\left(\mathbf{x}_{n+1} | \mathbf{z}_{n+1}\right) p\left(\mathbf{z}_{n+1} | \mathbf{z}_{n}\right) \\
                & =\sum_{\mathbf{z}_{n+1}} \beta(\mathbf{z}_{n+1})p(\mathbf{x}_{n+1}|\mathbf{z}_{n+1})p(\mathbf{z}_{n+1}|\mathbf{z}_n)\qquad (13.38)
                \end{aligned}$$</p>
                <p>ここでの初期条件は、(13.33)で $n=N$ と置くことによって、以下になります。$\mathbf{z}_n$ の全ての値について $\beta(\mathbf{z}_n) = 1$ とすることでこの式は正しくなります。
                $$p\left(\mathbf{z}_{N} | \mathbf{X}\right)=\frac{p\left(\mathbf{X}, \mathbf{z}_{N}\right) \beta\left(\mathbf{z}_{N}\right)}{p(\mathbf{X})}\qquad (13.39)$$
                </p>
              </div>
              <div class="column">
                <figure>
                  <img class="aligncenter" src="prml_static/images/Chap13/backward algorithm.png" alt="backward algorithm" width=50%>
                </figure>
                <br>
                <br>
                <p>$n+1$ ステップの $\beta(\mathbf{z_{n+1}})$ の要素 $\beta(z_{n+1},k)$ について重み付け和を取ることで、$\beta(z_{n,1})$ を得ます。ここで、各 $\beta(z_{n+1,k})$ に対する重みは、$p(\mathbf{z_{n+1}}|\mathbf{z}_n)$ の値に対応する $A_{1 k}$ とその要素に対応する出力密度 $p(\mathbf{x}_n|z_{n+1,k})$ の値を乗じたものです。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <p class="text-intro">$p(\mathbf{X})$ の計算</p>
                <p>$p(\mathbf{X})$ は尤度関数を表し、EMアルゴリズムの停止条件などに利用されることがあるのでその値を求めることは有用です。
                $$\gamma\left(\mathbf{z}_{n}\right)
                =\frac{p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}, \mathbf{z}_{n}\right) p\left(\mathbf{x}_{n+1}, \ldots, \mathbf{x}_{N} | \mathbf{z}_{n}\right)}{p(\mathbf{X})}
                =\frac{\alpha\left(\mathbf{z}_{n}\right) \beta\left(\mathbf{z}_{n}\right)}{p(\mathbf{X})}\qquad (13.33)$$
                の両辺において $\mathbf{z}_n$ について和を取り、左辺が正規化された分布であることを利用すると、
                $$p(\mathbf{X})=\sum_{\mathbf{z}_{n}} \alpha\left(\mathbf{z}_{n}\right) \beta\left(\mathbf{z}_{n}\right)\qquad (13.41)$$
                が得られます。すなわち、$n$ を好きなように取り、この和を計算することで尤度関数を求めることができます。よって、単に尤度関数を求めたい場合は $n=N$ の結果を用いて、以下で求めることが可能です。
                $$p(\mathbf{X})=\sum_{\mathbf{k}_{N}} \alpha\left(\mathbf{z}_{N}\right)\qquad (13.42)$$
              </div>
              <div class="column">
                <p class="text-intro">$\xi(\mathbf{z}_{n-1},\mathbf{z}_n)$ の計算</p>
                <p>$(\mathbf{z}_{n-1},\mathbf{z}_n)$ の $K\times K$ 個の値の組各々についての条件付き確率 $p(\mathbf{z}_{n-1},\mathbf{z}_n)$ に対応する $\xi(\mathbf{z}_{n-1},\mathbf{z}_n)$ は、ベイズの定理を用いることによって、以下で表されます。
                $$\begin{array}{l}
                {\xi\left(\mathbf{z}_{n-1}, \mathbf{z}_{n}\right)=p\left(\mathbf{z}_{n-1}, \mathbf{z}_{n} | \mathbf{X}\right)} \\
                {=\frac{p(\mathbf{X} | \mathbf{z}_{n-1}, \mathbf{z}_{n}) p\left(\mathbf{z}_{n-1}, \mathbf{z}_{n}\right)}{p(\mathbf{X})}} \\
                {=\frac{p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n-1} | \mathbf{z}_{n-1}\right) p\left(\mathbf{x}_{n} | \mathbf{z}_{n}\right) p\left(\mathbf{x}_{n+1}, \ldots, \mathbf{x}_{N} | \mathbf{z}_{n}\right) p\left(\mathbf{z}_{n} | \mathbf{z}_{n-1}\right) p\left(\mathbf{z}_{n-1}\right)}{p(\mathbf{X})}} \\
                {=\frac{\alpha\left(\mathbf{z}_{n-1}\right) p\left(\mathbf{x}_{n} | \mathbf{z}_{n}\right) p\left(\mathbf{z}_{n} | \mathbf{z}_{n-1}\right) \beta\left(\mathbf{x}_{n}\right)}{p(\mathbf{X})}} \qquad (13.43)
                \end{array}$$</p>
                <p>したがって、すでに計算した $\alpha(\mathbf{z}_n)$ と $\beta(\mathbf{z}_n)$ の値を用いて更新ができます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>まとめ</h2>
                <p class="text-intro">初期化</p>
                <p>最初に、パラメータ $\boldsymbol{\theta}^{\mathrm{old}}$ の初期値を求めます。</p>
                <p>パラメータ $\mathbf{A}$ や $\boldsymbol{\mu}$ は、一様に、もしくはランダムに、一様分布から非負性や和の制約を満たす範囲内で選択されることが多いです。</p>
                <p>パラメータ $\boldsymbol{\phi}$ の初期化は、出力分布の形により異なります。例えば、ガウス分布の場合には、データに K-means アルゴリズムを用いることによって $\boldsymbol{\mu}_k$ が初期化され、$\boldsymbol{\Sigma}_k$ の初期値としては対応する $K$ 個のクラスの共分散行列が用いられることがあります。</p>
                <p class="text-intro">E step</p>
                <p>まず、フォワード $\alpha$ 再帰とバックワード $\beta$ 再帰が実行され、その結果を用いて $\gamma(\mathbf{z}_n)$ と $\xi(\mathbf{z_{n-1},\mathbf{z}_n})$ を求めることができます。この段階で尤度関数も求めることができます。</p>
              </div>
              <div class="column">
                <p class="text-intro">M step</p>
                <p>上で求めたパラメータを固定し、
                $$\begin{aligned}
                Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text { old }}\right)
                =& \sum_{k=1}^{K} \gamma\left(z_{1 k}\right) \ln \pi_{k}+\sum_{n=2}^{N} \sum_{j=1}^{K} \sum_{k=1}^{K} \xi\left(z_{n-1, j}, z_{n k}\right) \ln A_{j k} \\
                & +\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right) \ln p\left(\mathbf{x}_{n} | \boldsymbol{\phi}_{k}\right)\qquad (13.17)
                \end{aligned}$$
                を最大化するようなパラメータ集合 $\boldsymbol{\theta}^{\mathrm{old}}$ を求めます。</p>
                <p class="text-intro">停止条件</p>
                <p>ある収束基準、例えば尤度関数の変化やパラメータの変化がある閾値より小さくなるまで、E step と M step を繰り返します。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>予測分布</h2>
                <p>データ $\mathbf{X} = \{\mathbf{x}_1,\ldots,\mathbf{x}_N\}$ が観測されたときに、$\mathbf{x}_{N+1}$ を予測したいことはよくあります。これは、ここまでと同様に、加法・乗法定理と条件付き独立性を用いることによって求められます。
                $$\begin{aligned}
                p\left(\mathbf{x}_{N+1} | \mathbf{X}\right)
                & =\sum_{\mathbf{z}_{N+1}} p\left(\mathbf{x}_{N+1}, \mathbf{z}_{N+1} | \mathbf{X}\right) \\
                & =\sum_{\mathbf{z}_{N+1}} p\left(\mathbf{x}_{N+1} | \mathbf{z}_{N+1}\right) p\left(\mathbf{z}_{N+1} | \mathbf{X}\right) \\
                & =\sum_{\mathbf{z}_{N+1}} p\left(\mathbf{x}_{N+1} | \mathbf{z}_{N+1}\right) \sum_{\mathbf{z}_{N}} p\left(\mathbf{z}_{N+1}, \mathbf{z}_{N} | \mathbf{X}\right) \\
                & =\sum_{\mathbf{z}_{N+1}} p\left(\mathbf{x}_{N+1} | \mathbf{z}_{N+1}\right) \sum_{\mathbf{z}_{N}} p\left(\mathbf{z}_{N+1} | \mathbf{z}_{N}\right) p\left(\mathbf{z}_{N} | \mathbf{X}\right) \\
                & =\sum_{\mathbf{z}_{N+1}} p\left(\mathbf{x}_{N+1} | \mathbf{z}_{N+1}\right) \sum_{\mathbf{z}_{N}} p\left(\mathbf{z}_{N+1} | \mathbf{z}_{N}\right) \frac{p\left(\mathbf{z}_{N}, \mathbf{X}\right)}{p(\mathbf{X})} \\
                & =\frac{1}{p(\mathbf{X})} \sum_{\mathbf{z}_{N+1}} p\left(\mathbf{x}_{N+1} | \mathbf{z}_{N+1}\right) \sum_{\mathbf{z}_{N}} p\left(\mathbf{z}_{N+1} | \mathbf{z}_{N}\right) \alpha\left(\mathbf{z}_{N}\right) \qquad (13.44)
                \end{aligned}$$</p>
              </div>
              <div class="column">
                <p>これは、まず最初にフォワード $\alpha$ 再帰を実行し、次に最後の $\mathbf{z}_N$ と $\mathbf{z}_{N+1}$ についての和を計算することで求められます。</p>
                <p>最初の $\mathbf{z}_N$ についての和を記憶しておくと、$\mathbf{x}_{N+1}$ が観測され、続く $\mathbf{x}_{N+2}$ の値を予測するために、さらにフォワード $\alpha$ 再帰を実行するときに用いることができます。</p>
                <p>つまり、(13.44)では、$\mathbf{x}_1$ から $\mathbf{x}_N$ までの全てのデータの影響が $\alpha(\mathbf{z}_N)$ の $K$ 個の値にまとめられています。</p>
                <p>つまり、限られた量の記憶領域を用いて無限の未来まで予測分布を計算し続けることができます。このことは、実行時間性が要求される応用で必要となります。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>Viterbiアルゴリズム</h2>
                <p>隠れマルコフモデルの多くの応用で、潜在変数は何らかの意味を持つと解釈されます。そこで、与えられた観測系列に対し、隠れ状態の最も確からしい系列を求めることにしばしば興味が持たれます。</p>
                <p>例えば、遺伝子配列解析で、隠れた状態から配列が決まっていると仮定すれば、その隠れた状態を見つけることに意味があると思われます。</p>
                <p>隠れマルコフモデルのグラフは有向木なので、この問題は<font color="red"><b>max-sumアルゴリズム</b></font>を用いることで厳密に解くことができます。</p>
                <p>ここで、先ほど利用した<font color="red"><b>Baum-Welch(積和)アルゴリズム</b></font>を用いて単純に潜在変数の周辺確率 $\gamma(\mathbf{z}_n)$ を求め、各 $n$ について最も確からしい状態の集合を求めるだけでは、一般には最も確からしい状態系列に対応しないことに注意が必要です。</p>
              </div>
              <div class="column">
                <p>この問題の定式化をすると、$\mathbf{Z}$ の事後確率が最大となる系列 $\hat{\mathbf{Z}}$ を見つけるという問題に対応するので、
                $$\begin{aligned}
                \hat{\mathbf{Z}}
                & = \mathop{\rm arg~max}\limits_{\mathbf{Z}}p(\mathbf{Z}|\mathbf{X}) \\
                & = \mathop{\rm arg~max}\limits_{\mathbf{Z}}\frac{p(\mathbf{X},\mathbf{Z})}{p(\mathbf{X})} \\
                & = \mathop{\rm arg~max}\limits_{\mathbf{Z}}p(\mathbf{X},\mathbf{Z}) \\
                & = \mathop{\rm arg~max}\limits_{\mathbf{Z}}\ln p(\mathbf{X},\mathbf{Z}) \\
                \end{aligned}$$を解くことになります。</p>
                <p>これを再帰的に計算するアルゴリズムを<font color="red"><b>ビタビアルゴリズム(Viterbi algorithm)</b></font>と呼びます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <p class="text-intro">Viterbiアルゴリズム</p>
                <p>まず、$\mathop{\rm max}\limits_{\mathbf{Z}}\ln p(\mathbf{X},\mathbf{Z})$ について考えます。</p>
                <p>これに、先ほど導いた
                $$p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta})=p\left(\mathbf{z}_{1} | \boldsymbol{\pi}\right)\left[\prod_{n=2}^{N} p\left(\mathbf{z}_{n} | \mathbf{z}_{n-1}, \mathbf{A}\right)\right] \prod_{m=1}^{N} p\left(\mathbf{x}_{m} | \mathbf{z}_{m}, \boldsymbol{\phi}\right)\qquad (13.10)$$
                を代入すると、以下の式を得ます。
                $$\mathop{\rm max}\limits_{\mathbf{Z}}\ln p(\mathbf{X},\mathbf{Z}) = \mathop{\rm max}\limits_{\mathbf{Z}} \left\{ \ln p\left(\mathbf{z}_{1} | \boldsymbol{\pi}\right) + \sum_{n=2}^N \ln p\left(\mathbf{z}_{n} | \mathbf{z}_{n-1}, \mathbf{A}\right) + \sum_{n=1}^N \ln p\left(\mathbf{x}_{m} | \mathbf{z}_{m}, \boldsymbol{\phi}\right)  \right\}$$</p>
                <p>そこで、以下のように $\omega\left(\mathbf{z}_{n}\right)$ を定義します。
                $$\omega\left(\mathbf{z}_{n}\right)=\max _{\mathbf{z}_{1}, \ldots, \mathbf{z}_{n-1}} p\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}, \mathbf{z}_{1}, \ldots, \mathbf{z}_{n}\right)\qquad (13.70)$$</p>
              </div>
              <div class="column">
                <p>すると、$\mathop{\rm max}\limits_{\mathbf{Z}} \ln p(\mathbf{X},\mathbf{Z}) = \mathop{\rm max}\limits_{\mathbf{z}_N}\omega\left(\mathbf{z}_{n}\right)$ を得ます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <p class="text-intro">Viterbiアルゴリズム</p>
                <p>ここで、$\omega$ は次の漸化式を満たします。
                $$\begin{aligned}
                \omega(\mathbf{z}_{n+1})
                & = \mathop{\rm max}\limits_{\mathbf{z}_1,\ldots,\mathbf{z}_n}\left\{ \ln p(\mathbf{z}_1) + \sum_{i=2}^{n+1}\ln p(\mathbf{z}_i|\mathbf{z}_{i-1}) + \sum_{i=1}^{n+1} \ln p(\mathbf{x}_i|\mathbf{z}_i)\right\} \\
                & = \mathop{\rm max}\limits_{\mathbf{z}_1,\ldots,\mathbf{z}_n}\left\{ \ln p(\mathbf{z}_1) + \sum_{i=2}^{n+1}\ln p(\mathbf{z}_i|\mathbf{z}_{i-1}) + \sum_{i=1}^{n+1} \ln p(\mathbf{x}_i|\mathbf{z}_i)\right\} \\
                & \qquad + \ln p(\mathbf{z}_{n+1}|\mathbf{z}_n) + \ln p(\mathbf{x}_{n+1}|\mathbf{z}_{n+1})\\
                & = \ln p(\mathbf{x}_{n+1}|\mathbf{z}_{n+1}) + \mathop{\rm max}\limits_{\mathbf{z}_n}\left\{\ln p(\mathbf{z}_{n+1}|\mathbf{z}_n) + \omega(\mathbf{z}_n)\right\}
                \end{aligned}$$</p>
                <p>これは、
                $$\omega\left(\mathbf{z}_{1}\right)=\ln p\left(\mathbf{z}_{1}\right)+\ln p\left(\mathbf{x}_{1} | \mathbf{z}_{1}\right)\qquad (13.69)$$
                によって初期化されます。</p>
              </div>
              <div class="column">
                <p>ここで、漸化式
                $$\omega(\mathbf{z}_{n+1}) = \ln p(\mathbf{x}_{n+1}|\mathbf{z}_{n+1}) + \mathop{\rm max}\limits_{\mathbf{z}_n}\left\{\ln p(\mathbf{z}_{n+1}|\mathbf{z}_n) + \omega(\mathbf{z}_n)\right\}$$
                において、$\mathbf{z}_{n+1}$ が決まると、$\max$ の部分の $\mathbf{z}_n$ の値が決まります。この関数関係 $\mathbf{z}_{n+1}\rightarrow\mathbf{z}_n$ を $\mathrm{pred}$ と書く事にします。つまり、
                $$\mathrm{pred}(\mathbf{z}_{n+1}) = \mathop{\rm arg~max}\limits_{\mathbf{z}_n}\left\{\ln p(\mathbf{z}_{n+1}|\mathbf{z}_n) + \omega(\mathbf{z}_n)\right\}$$
                となります。</p>
                <p>これを用いることで、左から右に、<font color="red"><b>「$\mathbf{z}_n = k$ の時に、そこより過去の部分の対数尤度が最大となるようなラベルの系列」</b></font>が計算されていきます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <p class="text-intro">Viterbiアルゴリズム</p>
                <p>求める隠れ変数の系列は
                $$\begin{aligned}
                \hat{\mathbf{Z}}
                & = \mathop{\rm arg~max}\limits_{\mathbf{Z}}\log p(\mathbf{X},\mathbf{Z}) \\
                & = \mathop{\rm arg~max}\limits_{\mathbf{z}_N}\omega(\mathbf{z}_N)
                \end{aligned}$$
                だったので、まず $\mathbf{z}_N$ を決めます。すると、$\mathrm{pred}$ によって $\mathbf{z}_{N-1} = \mathrm{pred}(\mathbf{z}_N)$ が決まります。</p>
                <p>これを繰り返すことで、系列 $\mathbf{z}_1 \rightarrow \mathbf{z}_2 \rightarrow \cdots \mathbf{z}_{N-1} \rightarrow \mathbf{z}_N$ が右から順番に求まることになります。いわゆる<font color="red"><b>バックトラッキング(backtracking)</b></font>という計算です。</p>
                <p>この時の計算過程を右に図示します。なお、赤いエッジが $\mathrm{pred:}\mathbf{z}_{n+1}\rightarrow\mathbf{z}_n$ に対応しており、濃い赤いエッジが求める系列となっています。</p>
              </div>
              <div class="column">
                <figure>
                  <img class="aligncenter" src="prml_static/images/Chap13/Viterbi algorithm.png" alt="Viterbi algorithm" style="background-color:white;">
                </figure>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2></h2>
                <p class="text-intro"></p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2></h2>
                <p class="text-intro"></p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2></h2>
                <p class="text-intro"></p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2></h2>
                <p class="text-intro"></p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2></h2>
                <p class="text-intro"></p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2></h2>
                <p class="text-intro"></p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2></h2>
                <p class="text-intro"></p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2></h2>
                <p class="text-intro"></p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2></h2>
                <p class="text-intro"></p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple aligncenter">
          <h2 class="text-emoji zoomIn">😊</h2>
          <h3><strong>Thank you!</strong></h2>
          <p><a href="https://twitter.com/cabernet_rock" title="@cabernet_rock on Twitter">@cabernet_rock</a></p>
        </section>

        <section class="bg-apple aligncenter">
          <!-- .wrap = container (width: 90%) -->
          <div class="wrap">
            <h2><strong>Please see my YouTube </strong></h2>
            <p class="text-intro">I'm explaining this slide.</p>
            <p>
              <a href="#" class="button" title="See YouTube">
                <svg class="fa-youtube">
                  <use xlink:href="#fa-youtube"></use>
                </svg>
                See my YouTube
              </a>
            </p>
          </div>
        </section>

      </article>
    </main>
    <!--main-->

    <!-- Required -->
    <script src="prml_static/js/webslides.js"></script>

    <script>
      window.ws = new WebSlides();
    </script>

    <!-- OPTIONAL - svg-icons.js (fontastic.me - Font Awesome as svg icons) -->
    <script defer src="prml_static/js/svg-icons.js"></script>

  </body>
</html>
