<!doctype html>
<html lang="ja">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../static/css/atom.min.css">
    <!--
      Hi source code lover!!

      I don't want to be a YouTuber.
      I want to make a platform where people can share and learn college knowledge one another.
      If you are interested it, please get in touch with me. (Twitter: @cabernet_rock)
    -->

    <!-- SEO -->
    <title>10 minutes PRML Chapter 5</title>
    <meta name="description" content="Learn Pattern Recognition and Machine Learning in 10 minutes.">

    <!-- URL CANONICAL -->
    <!-- <link rel="canonical" href="http://your-url.com/permalink"> -->

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,700,700i%7CMaitree:200,300,400,600,700&amp;subset=latin-ext" rel="stylesheet">

    <!-- CSS Base -->
    <link rel="stylesheet" type='text/css' media='all' href="prml_static/css/webslides.css">

    <!-- Optional - CSS SVG Icons (Font Awesome) -->
    <link rel="stylesheet" type="text/css" media="all" href="prml_static/css/svg-icons.css">

    <!-- SOCIAL CARDS (Open Graph protocol) -->
    <!-- FACEBOOK -->
    <meta property="og:url" content="http://your-url.com/permalink">
    <meta property="og:type" content="article">
    <meta property="og:title" content="10 minutes PRML Chapter 5">
    <meta property="og:description" content="Learn Pattern Recognition and Machine Learning in 10 minutes.">
    <meta property="og:image" content="prml_static/images/share-webslides.jpg" >

    <!-- TWITTER -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:creator" content="@cabernet_rock">
    <meta name="twitter:title" content="10 minutes PRML Chapter 5">
    <meta name="twitter:description" content="Learn Pattern Recognition and Machine Learning in 10 minutes.">
    <meta name="twitter:image" content="prml_static/images/share-webslides.jpg">

    <!-- FAVICONS -->
    <link rel="shortcut icon" sizes="16x16" href="prml_static/images/favicons/favicon.png">
    <link rel="shortcut icon" sizes="32x32" href="prml_static/images/favicons/favicon-32.png">
    <link rel="apple-touch-icon icon" sizes="76x76" href="prml_static/images/favicons/favicon-76.png">
    <link rel="apple-touch-icon icon" sizes="120x120" href="prml_static/images/favicons/favicon-120.png">
    <link rel="apple-touch-icon icon" sizes="152x152" href="prml_static/images/favicons/favicon-152.png">
    <link rel="apple-touch-icon icon" sizes="180x180" href="prml_static/images/favicons/favicon-180.png">
    <link rel="apple-touch-icon icon" sizes="192x192" href="prml_static/images/favicons/favicon-192.png">

    <!-- Android -->
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="theme-color" content="#333333">

    <!-- Syntax highlight -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!-- Tex -->
    <!-- Local env -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- Github env -->
    <!--
    <script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
      });
    </script>
  </head>

  <body>
    <header role="banner">
      <nav role="navigation">
        <ul>
          <li class="github">
            <a rel="external" href="#" title="YouTube">
              <svg class="fa-youtube">
                <use xlink:href="#fa-youtube"></use>
              </svg>
              <em>Colledge Knowledge</em>
            </a>
          </li>
          <li class="twitter">
            <a rel="external" href="https://twitter.com/cabernet_rock" title="Twitter">
              <svg class="fa-twitter">
                <use xlink:href="#fa-twitter"></use>
              </svg>
              <em>@cabernet_rock</em>
            </a>
          </li>
        </ul>
      </nav>
    </header>

    <main role="main">
      <article id="webslides">

        <!-- Quick Guide
          - Each parent <section> in the <article id="webslides"> element is an individual slide.
          - Vertical sliding = <article id="webslides" class="vertical">
          - <div class="wrap"> = container 90% / <div class="wrap size-50"> = 45%;
        -->

        <section class="bg-apple">
          <h1>§5 Neural Networks</h1>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>単純パーセプトロン</h2>
                <p class="text-intro">perceptron</p>
                <p>ニューラルネットワークは多数の素子（パーセプトロン）の集まりで表現されます。そこで、まずは一つ一つの素子を見ていきます。</p>
                <p>一つの素子は右の図のように表され、入力を受け取り、その値がある閾値 $\theta$ を超えたら発火して出力し、$\theta$ 以下だったら出力しない、という性質を持ちます。数学的に記述すると、</p>
                <ul class="description">
                  <li>重みパラメータ：$w_0,w_1,\ldots,w_m$</li>
                  <li>活性化関数：$h$</li>
                </ul>
                <p>から構成され、以下のような関数を表します。
                $$f(x_1,x_2,\ldots,x_m) = h(w_1x_1+w_2x_2+\cdots+w_mx_m+w_0)$$</p>
              </div>
              <div class="column">
                <figure>
                  <img src="prml_static/images/Chap4/iris_man.png" alt="iris_man">
                </figure>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>重みパラメータ</h2>
                <p class="text-intro">weight parameter</p>
                <p>今までに出て来た重み $\mathbf{w}$ と同じ役割をします。入力 $x_i$ の重要度を $w_i$ が表しているとも言えます。</p>
                <h2>活性化関数</h2>
                <p class="text-intro">activation function</p>
                <p>ある閾値 $\theta$ を超えたら発火して出力し、$\theta$ 以下だったら出力しない、という神経の性質を表現するため、以下の<b><font color="red">ヘヴィサイド関数(Heaviside function)</font></b>などが利用されます。
                $$h(a) = \left\{\begin{array}{cc}0 & (\theta < 0) \\1 & (\theta > 0)\end{array}\right.$$</p>
              </div>
              <div class="column">
                <p>この性質は表したい能力を如実に表していますが、不連続であり扱いづらいため、ロジスティックシグモイド関数
                $$h(a) = \frac{1}{1+\exp(-a)}$$や、それと等価な能力を持つハイパボリックタンジェント関数
                $$h(a) = \tanh (a)$$ソフトマックス関数
                $$h(\mathbf{a}) = \frac{\exp(a_i)}{\sum_j \exp(a_i)}$$などが用いられることが多いです。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>多層パーセプトロン</h2>
                <p class="text-intro">Multilayer Perceptron</p>
                <p>それでは、一つの単子（単純パーセプロトン）を繋げて２層のパーセプトロンを作って見ましょう。なお、２層以上のパーセプトロンを<font color="red"><b>ニューラルネットワーク</b></font>と言います。</p>
                <p>例えば１層目に $m$ 個の素子を用意し、入力が $D$ 次元の場合、
                $$y = h_2\left(\sum_{i=0}^{m} w^{(2)}_ih_1\left(\sum_{j=0}^{D}w^{(1)}_{ij} x_j\right)\right)$$となります。</p>
                <p>ただし、それぞれの記号の意味は以下の通りです。
                  <li>$w^{(1)}_{ij}$ は１層目の $i$ 番目の素子の入力 $j$ の重み
                  <li>$w^{(2)}_i$ は２層目の素子の入力 $i$ の重み
                  <li>$h_1,h_2$ はそれぞれの層の活性化関数
                </p>
              </div>
              <div class="column">
                <figure>
                  <img src="prml_static/images/Chap4/iris_man.png" alt="iris_man">
                </figure>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>多層パーセプトロン</h2>
                <p class="text-intro">多層にする意味</p>
                <p>この時何が起こるのでしょうか？線形識別モデルと先ほど導出した２層のパーセプトロンを比べて見ます。</p>
                <li>線形識別モデル
                  $$ y=f\left(\sum_i w_i\color{red}\phi_i(\mathbf{x})\right) $$
                </li>
                <li>２層のパーセプトロン
                  $$ y = h_2\left(\sum_{i=0}^{m} w^{(2)}_i\color{red}h_1\left(\sum_{j=0}^{D}w^{(1)}_{ij} x_j\right)\right) $$
                </li>
              </div>
              <div class="column">
                <p>これより、基底 $\phi_i(\mathbf{x})$ が $\displaystyle h_1\left(\sum_{j=0}^{D}w^{(1)}_{ij} x_j\right)$ に置き換わっていることがわかります。</p>
                <p>どういうことかというと、<font color="red"><b>今まで固定されていた基底関数が、多層になることで適応的に変動するようになる</b></font>ということです。</p>
                <p>これより、十分大きな $m$ をとった多層パーセプトロンは、任意の関数を任意の精度で近似することができる、という性質を持ちます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>誤差逆伝播法</h2>
                <p class="text-intro">back propagation</p>
                <p>ニューラルネットワークについて語る上で避けては通れないのが<font color="red"><b>誤差逆伝播法（back propagation）</b></font>です。が、ここでは簡単にだけ説明をして流します。</p>
                <p>というのも、ただの「合成関数の微分則」だからです。</p>
                <p>ニューラルネットワークを訓練する時、訓練データの出力と正解の誤差 $E_n(\mathbf{w})$ を最小化したいと考えます。</p>
                <p>この時、素子 $i$ から素子 $j$ への接続の重みを $w_{ji}$ として、 全ての $i,j$ の組合せ（＝全ノードの重み）に対して$ \frac{\partial E_n}{\partial w_{ji}} $を計算する事が目標となります</p>
              </div>
              <div class="column">
                <p>
                  <li>素子 $i$ の出力を $z_i$</li>
                  <li>素子 $j$ への入力和を $a_j$</li>
                </p>
                <p>とすると、「$E_n$ は $a_j$ を介してのみ $w_{ji}$ に依存する」（＝ $w_{ji}$ が変化すると $a_j$ が変化し、それが $E_n$ に影響を及ぼす）事に注意すれば
                $$ \frac{\partial E_n}{\partial w_{ji}} = \frac{\partial E_n}{\partial a_j}\frac{\partial a_j}{\partial w_{ji}}$$
                となります。</p>
                <p>さらに、$\delta_j = \partial E_n/\partial a_j$ と書くことにし、これを誤差と呼びます。</p>
                <p>すると、$\delta_i$ について考えると、 $E_n$ は素子 $i$ の出力を受け取る素子 $j$ の入力 $a_j$ を介して $a_i$ に依存するので合成微分則より
                $$ \delta_i = \frac{\partial E_n}{\partial a_i} = \sum_j \frac{\partial E_n}{\partial a_j}\frac{\partial a_j}{\partial a_i} = \sum_j\delta_j\frac{\partial a_j}{\partial a_i}$$
                となります。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>誤差逆伝播法</h2>
                <p class="text-intro">back propagation</p>
                <p>さらに素子 $i$ </p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>


        <section class="bg-apple aligncenter">
          <h2 class="text-emoji zoomIn">😊</h2>
          <h3><strong>Thank you!</strong></h2>
          <p><a href="https://twitter.com/cabernet_rock" title="@cabernet_rock on Twitter">@cabernet_rock</a></p>
        </section>

        <section class="bg-apple aligncenter">
          <!-- .wrap = container (width: 90%) -->
          <div class="wrap">
            <h2><strong>Please see my YouTube </strong></h2>
            <p class="text-intro">I'm explaining this slide.</p>
            <p>
              <a href="#" class="button" title="See YouTube">
                <svg class="fa-youtube">
                  <use xlink:href="#fa-youtube"></use>
                </svg>
                See my YouTube
              </a>
            </p>
          </div>
        </section>

      </article>
    </main>
    <!--main-->

    <!-- Required -->
    <script src="prml_static/js/webslides.js"></script>

    <script>
      window.ws = new WebSlides();
    </script>

    <!-- OPTIONAL - svg-icons.js (fontastic.me - Font Awesome as svg icons) -->
    <script defer src="prml_static/js/svg-icons.js"></script>

  </body>
</html>
