<!doctype html>
<html lang="ja">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../static/css/atom.min.css">
    <!--
      Hi source code lover!!

      I don't want to be a YouTuber.
      I want to make a platform where people can share and learn college knowledge one another.
      If you are interested it, please get in touch with me. (Twitter: @cabernet_rock)
    -->

    <!-- SEO -->
    <title>10 minutes PRML Chapter 10</title>
    <meta name="description" content="Learn Pattern Recognition and Machine Learning in 10 minutes.">

    <!-- URL CANONICAL -->
    <!-- <link rel="canonical" href="http://your-url.com/permalink"> -->

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,700,700i%7CMaitree:200,300,400,600,700&amp;subset=latin-ext" rel="stylesheet">

    <!-- CSS Base -->
    <link rel="stylesheet" type='text/css' media='all' href="prml_static/css/webslides.css">

    <!-- Optional - CSS SVG Icons (Font Awesome) -->
    <link rel="stylesheet" type="text/css" media="all" href="prml_static/css/svg-icons.css">

    <!-- SOCIAL CARDS (Open Graph protocol) -->
    <!-- FACEBOOK -->
    <meta property="og:url" content="https://iwasakishuto.github.io">
    <meta property="og:type" content="article">
    <meta property="og:title" content="10 minutes PRML Chapter 10">
    <meta property="og:description" content="Learn Pattern Recognition and Machine Learning in 10 minutes.">
    <meta property="og:image" content="prml_static/images/share-webslides.jpg" >

    <!-- TWITTER -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:creator" content="@cabernet_rock">
    <meta name="twitter:title" content="10 minutes PRML Chapter 10">
    <meta name="twitter:description" content="Learn Pattern Recognition and Machine Learning in 10 minutes.">
    <meta name="twitter:image" content="prml_static/images/share-webslides.jpg">

    <!-- FAVICONS -->
    <link rel="shortcut icon" sizes="16x16" href="prml_static/images/favicons/favicon.png">
    <link rel="shortcut icon" sizes="32x32" href="prml_static/images/favicons/favicon-32.png">
    <link rel="apple-touch-icon icon" sizes="76x76" href="prml_static/images/favicons/favicon-76.png">
    <link rel="apple-touch-icon icon" sizes="120x120" href="prml_static/images/favicons/favicon-120.png">
    <link rel="apple-touch-icon icon" sizes="152x152" href="prml_static/images/favicons/favicon-152.png">
    <link rel="apple-touch-icon icon" sizes="180x180" href="prml_static/images/favicons/favicon-180.png">
    <link rel="apple-touch-icon icon" sizes="192x192" href="prml_static/images/favicons/favicon-192.png">

    <!-- Android -->
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="theme-color" content="#333333">

    <!-- Syntax highlight -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!-- Tex -->
    <!-- Local env -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- Github env -->
    <!--
    <script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
      });
    </script>
  </head>

  <body>
    <header role="banner">
      <nav role="navigation">
        <ul>
          <li class="github">
            <a rel="external" href="#" title="YouTube">
              <svg class="fa-youtube">
                <use xlink:href="#fa-youtube"></use>
              </svg>
              <em>Colledge Knowledge</em>
            </a>
          </li>
          <li class="twitter">
            <a rel="external" href="https://twitter.com/cabernet_rock" title="Twitter">
              <svg class="fa-twitter">
                <use xlink:href="#fa-twitter"></use>
              </svg>
              <em>@cabernet_rock</em>
            </a>
          </li>
        </ul>
      </nav>
    </header>

    <main role="main">
      <article id="webslides">

        <!-- Quick Guide
          - Each parent <section> in the <article id="webslides"> element is an individual slide.
          - Vertical sliding = <article id="webslides" class="vertical">
          - <div class="wrap"> = container 90% / <div class="wrap size-50"> = 45%;
        -->

        <section class="bg-apple">
          <h1>§10 Approximate Inference</h1>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>近似推論法</h2>
                <p>確率モデルを運用する際に中心的となるタスクは、観測データ $\mathbf{X}$ が与えられた時の潜在変数 $\mathbf{Z}$ の事後分布 $p(\mathbf{Z}|\mathbf{X})$ を求めること、およびこの分布を用いて期待値を求めることです。</p>
                <p>しかし、実際に興味のある多くのモデルでは、事後分布を求めることや、その事後分布に従った期待値を計算することは不可能なことが多いです。</p>
                <p>これは、隠れ変数の空間全体を直接扱うには次元が高すぎることや、事後分布が複雑な形をしており、期待値が解析的に計算できないことが理由です。</p>
              </div>
              <div class="column">
                <p>そこで、以下の二種類の近似法を用いることで、この問題に対処します。</p>
                <ul class="description">
                  <li>
                    <span class="text-label">確率的手法</span>
                    無限の計算資源があれば厳密な結果を計算することができるが実際には計算量が多くなってしまうので、スケールの小さな問題にしか用いられない。「MCMC法」など。11章で扱います。
                  </li>
                  <li>
                    <span class="text-label">決定的手法</span>
                    事後分布が特定の方法で分解されることや、パラメトリックな分布となることを仮定し、事後分布を解析的に求める手法。ここでは、「変分推論」「EP法」を考えます。。
                  </li>
                </ul>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分法</h2>
                <p class="text-intro">【問題】</p>
                <p>$\mathbb{R}$ 上の連続分布 $p(x)$ で平均が $0$, 分散が $1$ であるもののうち、エントロピー $I[p] = -\int p(x)\log p(x)\mathrm{d} x$ が最大になるようなものは何でしょうか？</p>
                <p class="text-intro">【解答】</p>
                <p>通常の極値問題と同じように、<font color="red"><b>「$I[p]$ が極値ならば、$p$ に微小変化を加えても $I[p]$ は変化しない」</b></font>という事実を用います。</p>
                <p>しかし、ここで問題となるのは、 $p(x)$ が関数のため、<font color="red"><b>微小変化の方法が無限通りある</b></font>ことです。そこで、関数 $\eta (x)$ （＝ずらす方向）を任意に固定し、$\varepsilon$ を実数値変数として
                $$p(x) \rightarrow p(x) + \varepsilon \times \eta(x)$$と書けるものだけを考え、$\varepsilon = 0$ 付近で変化率が $0$ になる条件を求めます。</p>
              </div>
              <div class="column">
                <p>これは、「点 $p$ での方向微分係数が、任意の方向ベクトル $\eta$ に関して $0$ となれば良い、ということです。」</p>
                <p>確率の和が $1$、平均が $0$、分散が $1$ という制約条件があるので、ラグランジュの未定乗数 $\lambda_1,\lambda_2,\lambda_3$ を導入して、
                $$\begin{aligned}F[p]
                & = -\int p(x)\log p(x)\mathrm{d} x + \lambda_1\left(\int p(x)\mathrm{d}x - 1\right) \\
                & + \lambda_2\int xp(x)\mathrm{d}x + \lambda_3\left(\int x^2p(x)\mathrm{d}x - 1 \right)\\
                & = - \int \left\{ \log p(x) - \lambda_1 - \lambda_2 x- \lambda_3 x^2\right\}p(x)\mathrm{d} x - \lambda_1 - \lambda_3\end{aligned}$$
                が停留点を取る条件を求めます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分法</h2>
                <p>ここで、$p$ に微小変化を加えると、
                $$F[p] = - \int \left\{ \log \color{red}{(p+\varepsilon\eta)} - \lambda_1 - \lambda_2 x- \lambda_3 x^2\right\}\color{red}{(p+\varepsilon\eta)}\mathrm{d} x - \lambda_1 - \lambda_3$$
                となります。これを $\varepsilon$ で微分すると、
                $$- \int\left\{ \log (p+\varepsilon\eta) - \lambda_1 - \lambda_2 x- \lambda_3 x^2\right\}\eta\mathrm{d} x  - \int \eta\mathrm{d} x $$
                となり、$\varepsilon=0$ でこれが $0$ となる事が必要です。よって、
                $$- \int\left\{ \log p - \lambda_1 - \lambda_2 x- \lambda_3 x^2 + 1 \right\}\eta\mathrm{d} x  = 0 $$
                が任意の $\eta(x)$ で成立するので、
                $$\log p - \lambda_1 - \lambda_2 x- \lambda_3 x^2 + 1 = 0 \Leftrightarrow p(x) = \exp\left\{\lambda_1+\lambda_2 x+\lambda_3 x^2-1\right\}$$
                となります。</p>
              </div>
              <div class="column">
                <p>発散しないためには、$x$ の取りうる値の範囲が $-\infty$ 〜 $+\infty$ だから、$\lambda_3 < 0$ が必要なことに注意します。</p>
                <p>ここで、
                $$\int p'(x)\mathrm{d} x = \left[p(x)\right]_{-\infty}^{\infty} = 0$$
                ですが、
                $$\text{(左辺)} = \int \{\lambda_2 + 2\lambda_3 x\}p(x)\mathrm{d}x = \lambda_2+2\lambda_3\times 0 = \lambda_2$$
                なので、$\lambda_2 = 0$ です。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分法</h2>
                <p>すると、制約条件より、
                $$\int p(x)\mathrm{d} x = \sqrt{\frac{\pi}{-\lambda_3}}e^{\lambda_1-1}= 1 \Leftrightarrow e^{\lambda_1-1} = \sqrt{\frac{-\lambda_3}{\pi}}$$
                から
                $$p(x) = \sqrt{\frac{-\lambda_3}{\pi}}\exp(\lambda_3 x^2)$$
                となり、さらに
                $$\int x^2p(x)\mathrm{d} x = \frac{1}{-2\lambda_3} = 1\Leftrightarrow \lambda_3 = -\frac{1}{2}$$
                となります。</p>
              </div>
              <div class="column">
                <p>以上より、平均が $0$ で分散が $1$ の連続分布でエントロピー
                $$H[p]=\int p(x) \ln p(x) \mathrm{d} x\qquad (10.1)$$
                が最大となる分布は、標準正規分布
                $$\mathcal{N}(x|0,1) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^2}{2}\right)$$
                であることが証明されました。</p>
                <p>なお、平均を $\mu$、分散を $\sigma^2$ と一般化した場合や多次元の場合も全く同様に証明が可能です。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分ベイズ</h2>
                <p class="text-intro">variational Bayes</p>
                <p>それでは、変分最適化を推論問題に適用する流れを見ていきます。</p>
                <p>全てのパラメータが事前分布を与えられた、完全にベイズ的なモデルがあるとします。モデルには形状を表すパラメータの他に潜在変数がある可能性があり、それらを全て確率変数 $\mathbf{Z}$ と置きます。</p>
                <p>すると、確率モデルによって同時分布 $p(\mathbf{X},\mathbf{Z})$ が定められるので、事後分布 $p(\mathbf{Z}|\mathbf{X})$ およびモデルエビデンス $p(\mathbf{X})$ の近似を求めることが目標となります。</p>
                <p>前回のEM法の議論と同じように、周辺分布の対数は、
                $$\ln p(\mathbf{X})=\mathcal{L}(q)+\mathrm{KL}(q \| p)\qquad (10.2)$$
                と分解できます。</p>
              </div>
              <div class="column">
                <p>ここで、
                $$\begin{aligned}
                \mathcal{L}(q)
                & =\int q(\mathbf{Z}) \ln \left\{\frac{p(\mathbf{X}, \mathbf{Z})}{q(\mathbf{Z})}\right\} \mathrm{d} \mathbf{Z}
                & (10.3)\\
                \mathrm{KL}(q \| p)
                & =-\int q(\mathbf{Z}) \ln \left\{\frac{p(\mathbf{Z} | \mathbf{X})}{q(\mathbf{Z})}\right\} \mathrm{d} \mathbf{Z}
                & (10.4)
                \end{aligned}$$としました。</p>
                <p>これがEM法の議論と違っているのは、パラメータベクトル $\mathbf{\theta}$ が現れず、確率変数として $\mathbf{Z}$ の中に含まれていることです。</p>
                <p>$\ln p(\mathbf{X})$ を最大にするためには下界 $\mathcal{L}(q)$ を $q$ に関して最適化して最大化すればよく、これは $KL$ ダイバージェンスを最小化することと同値です。</p>
                <p>もし $q(\mathbf{Z})$ を任意の分布にして良ければ下界の最大化は $KL$ ダイバージェンスが $0$ になる時に起こり、その時 $\mathrm{KL}(q||p)=0 \Leftrightarrow \color{red}{ p(\mathbf{Z}|\mathbf{X}) = q(\mathbf{Z}) } $ より $q(\mathbf{Z})$ は真の事後分布 $p(\mathbf{Z}|\mathbf{X})$ になります。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>平均場近似</h2>
                <p class="text-intro">mean field approximation</p>
                <p>しかし、$q(\mathbf{Z})$ を任意の分布から探索する事が難しい事が往々にしてあります。</p>
                <p>そこで、$q(\mathbf{Z})$ のクラスを制限するために、$\mathbf{Z}$ をいくつかの排反なグループ $\mathbf{Z}_i(i=1,\ldots,M)$ に分割し
                $$q(\mathbf{Z})=\prod_{i=1}^{M} q_{i}\left(\mathbf{Z}_{i}\right)\qquad (10.5)$$
                といくつかの分布の積に分解できる場合のみを考えるという<font color="red"><b>平均場近似(mean field approximation)</b></font>を考えます。</p>
                <p>この近似では分解可能性のみを仮定し、各 $q_i(\mathbf{Z}_i)$ がどのような分布であるかは一切仮定していません。</p>
              </div>
              <div class="column">
                <p>また、このように分布のクラスを制限することは、過学習を抑えるという良い効果もあります。</p>
                <p>なお、平均場(mean field)という用語は物理学に由来しており、元々は解析的に解けない多体問題を、一体問題や二体問題などの解析的に解く事ができる問題に帰着させるための近似モデルです。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>平均場近似</h2>
                <p class="text-intro">mean field approximation</p>
                <p>(10.5)の形を持つ分布 $q(\mathbf{Z})$ の中で、下界 $\mathcal{L}(q)$ が最大になるものを探します。まずは、(10.5)を(10.3)に代入し、因子の一つ $q_j(\mathbf{Z}_j)$ に対する依存項を取り出して見ると、
                $$\begin{aligned}
                \mathcal{L}(q)
                & =\int \prod_{i} q_{i}\left\{\ln p(\mathbf{X}, \mathbf{Z})-\sum_{i} \ln q_{i}\right\} \mathrm{d} \mathbf{Z} \\
                & =\int q_{j}\left\{\int \ln p(\mathbf{X}, \mathbf{Z}) \prod_{i \neq j} q_{i} \mathrm{d} \mathbf{Z}_{i}\right\} \mathrm{d} \mathbf{Z}_{j}-\int q_{j} \ln q_{j} \mathrm{d} \mathbf{Z}_{j}+\mathrm{const} \\
                & =\int q_{j} \ln \tilde{p}\left(\mathbf{X}, \mathbf{Z}_{j}\right) \mathrm{d} \mathbf{Z}_{j}-\int q_{j} \ln q_{j} \mathrm{d} \mathbf{Z}_{j}+\mathrm{const} \qquad (10.6)
                \end{aligned}$$
                を得ます。なお、記法を簡単にするために $q_j(\mathbf{Z}_j)$ を $q_j$ と書き、新しい分布 $\tilde{p}\left(\mathbf{X}, \mathbf{Z}_{j}\right)$ を、</p>
              </div>
              <div class="column">
                <p>$$\ln \widetilde{p}\left(\mathbf{X}, \mathbf{Z}_{j}\right)=\mathbb{E}_{i \neq j}[\ln p(\mathbf{X}, \mathbf{Z})]+\text { const. }\qquad (10.7)$$
                で書きました。また、記法 $\mathbb{E}_{i \neq j}[\cdots]$ は、$i\neq j$ である全ての $\mathbf{Z}_i$ による分布 $q$ での期待値を表すので、
                $$\mathbb{E}_{i \neq j}[\ln p(\mathbf{X}, \mathbf{Z})]=\int \ln p(\mathbf{X}, \mathbf{Z}) \prod_{i \neq j} q_{i} \mathrm{d} \mathbf{Z}_{i}\qquad (10.8)$$
                です。ここで、
                $$\begin{aligned}
                \mathcal{L}(q)
                & = \int q_j \ln \tilde{p}(\mathbf{X},\mathbf{Z}_j)\mathrm{d}\mathbf{Z}_j - \int q_j \ln q_j\mathrm{d}\mathbf{Z}_j + \mathrm{const}. \\
                & = \int q_j(\mathbf{Z}_j)\ln\frac{\tilde{p}(\mathbf{X},\mathbf{Z}_j)}{q_j(\mathbf{Z}_j)}\mathrm{d}\mathbf{Z}_j + \mathrm{const}. \\
                & = -\mathrm{KL}(q_j || \tilde{p}) + \mathrm{const}.\end{aligned}$$
                より、(10.6)が $q_j(\mathbf{Z}_j)$ と $\tilde{p}(\mathbf{X},\mathbf{Z}_j)$ の間の負の $KL$ ダイバージェンスであるので、(10.6)の最大化は $KL$ ダイバージェンスの最小化と同値であり、その時以下が成立します。
                $$\ln q_{j}^{\star}\left(\mathbf{Z}_{j}\right)=\ln\tilde{p}(\mathbf{X},\mathbf{Z}_j) = \mathbb{E}_{i \neq j}[\ln p(\mathbf{X}, \mathbf{Z})]+\text { const. }\qquad (10.9)$$</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>平均場近似</h2>
                <p class="text-intro">mean field approximation</p>
                <p>以上より、$q_i(i\neq j)$ を固定して $q_j$ のみを動かした時に $\mathcal{L}(q)$ が最大となる条件はカルバックライブラー情報量 $KL(q_j)||\tilde{p} = 0$ となる条件で、それは $q_j(\mathbf{Z}_j) = \tilde{p}(\mathbf{X},\mathbf{Z}_j)$ となります。明示的に書き出すと、
                   $$q_{j}^{\star}\left(\mathbf{Z}_{j}\right)=\frac{\exp \left(\mathbb{E}_{i \neq j}[\ln p(\mathbf{X}, \mathbf{Z})]\right)}{\int \exp \left(\mathbb{E}_{i \neq j}[\ln p(\mathbf{X}, \mathbf{Z})]\right) \mathrm{d} \mathbf{Z}_{j}}$$
                   となり、この右辺の期待値計算は他の因子 $q_i(\mathbf{Z}_i)(i\neq j)$ に依存しているため、反復計算が必要となります。</p>
              </div>
              <div class="column">
                <p class="text-intro">まとめ</p>
                <p>観測変数 $\mathbf{X}$、隠れ変数 $\mathbf{Z}$ に対する同時分布のモデル $p(\mathbf{X},\mathbf{Z})$ について $\mathbf{Z}$ の事後分布 $p(\mathbf{Z}|\mathbf{X})$ を計算するためには、$\mathbf{Z}$ を幾つかの変数群 $\mathbf{Z}_i$ に分割し、それらの分布 $q_i(\mathbf{Z}_i)$ を適当に初期化し、
                $$q_{j}^{\mathrm{new}}\left(\mathbf{Z}_{j}\right) \propto \exp \left\{\mathbb{E}_{i \neq j}[\log p(\mathbf{X}, \mathbf{Z})]\right\}$$
                によって更新することを、収束するまで行います。</p>
                <p>この時求めた $q_i(\mathbf{Z}_i)$ が、事後分布 $p(\mathbf{Z}_i|\mathbf{X})$ を近似する分布となります。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>分解による近似の持つ性質</h2>
                <p>それでは、分布を分解によって近似することでどんな不正確さを生じさせてしまうかを考えます。</p>
                <p>まず、ガウス分布を分解されたガウス分布で近似する問題を考えます。</p>
                <p>相関のある二つの変数 $\mathbf{z} = (z_1,z_2)$ についてのガウス分布 $p(\mathbf{z})$ についてのガウス分布 $p(\mathbf{z})=\mathcal{N}(\mathbf{z} | \boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1})$ を考えます。この平均と精度行列を要素で書くと、
                  $$\boldsymbol{\mu}=\left( \begin{array}{c}{\mu_{1}} \\ {\mu_{2}}\end{array}\right), \quad \boldsymbol{\Lambda}=\left( \begin{array}{cc}{\Lambda_{11}} & {\Lambda_{12}} \\ {\Lambda_{21}} & {\Lambda_{22}}\end{array}\right) \qquad (10.10)$$となります。なお、精度行列の対称性から、$\Lambda_{21} = \Lambda_{12}$ となっています。</p>
              </div>
              <div class="column">
                <p>今、この分布を、分解したガウス分布 $q(\mathbf{z}) = q(z_1)q(z_2)$ で近似します。</p>
                <p>最初に、一般的な結果(10.9)を使って最適な因子 $q_{1}^{\star}(z_{1})$ を求めます。このためには、<font color="red"><b>$z_1$ に依存する項だけを考えれば良い</b></font>ことに注意します。他の項は全て正規化定数に含まれてしまうからです。したがって、
                  $$\begin{aligned}
                  \ln q_{1}^{\star}(z_{1})
                  & =\mathbb{E}_{z_{2}}[\ln p(\mathbf{z})]+\text { const } \\
                  & =\mathbb{E}_{z_{2}}\left[-\frac{1}{2}\left(z_{1}-\mu_{1}\right)^{2} \Lambda_{11}-\left(z_{1}-\mu_{1}\right) \Lambda_{12}\left(z_{2}-\mu_{2}\right)\right]+\mathrm{const} \\
                  & =-\frac{1}{2} z_{1}^{2} \Lambda_{11}+z_{1} \mu_{1} \Lambda_{11}-z_{1} \Lambda_{12}\left(\mathbb{E}\left[z_{2}\right]-\mu_{2}\right)+\mathrm{const.} \qquad (10.11)
                  \end{aligned}$$を得ます。</p>
                  <p>ここで、この式の右辺が $z_1$ の二次関数になっていることに注意すると、$q_{1}^{\star}(z_{1})$ がガウス分布である事がわかります。この結果は何も仮定をおかずに出てきたことを強調しておきます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <p>先の式を変形すると、
                $$\begin{aligned}
                \ln q_{1}^{\star}(z_{1})
                & = -\frac{1}{2} z_{1}^{2} \Lambda_{11}+z_{1} \mu_{1} \Lambda_{11}-z_{1} \Lambda_{12}\left(\mathbb{E}\left[z_{2}\right]-\mu_{2}\right)+\mathrm{const.} \qquad (10.11)\\
                & = -\frac{1}{2}\Lambda_{11} \left( z_1^2- \left( \mu_1-\frac{\Lambda_{12}}{\Lambda_{11}}\left(\mathbb{E}\left[z_{2}\right]-\mu_{2}\right)z_1 \right) \right)\\
                & = -\frac{1}{2}\Lambda_{11} \left( z_1  - \left( \mu_1-\Lambda_{11}^{-1}\Lambda_{12}\left(\mathbb{E}\left[z_{2}\right]-\mu_{2}\right) \right) \right)^2 +\mathrm{const.}
                \end{aligned}$$となるので、
                $$q_{1}^{\star}\left(z_{1}\right)=\mathcal{N}\left(z_{1} | m_{1}, \Lambda_{11}^{-1}\right)\qquad (10.12)$$
                $$m_{1}=\mu_{1}-\Lambda_{11}^{-1} \Lambda_{12}\left(\mathbb{E}\left[z_{2}\right]-\mu_{2}\right)\qquad (10.13)$$
                となります。この時、対称性から $q_{2}^{\star}(z_{2})$ もガウス分布になる事がわかり、以下のようになります。
                $$q_{2}^{\star}\left(z_{2}\right)=\mathcal{N}\left(z_{2} | m_{2}, \Lambda_{22}^{-1}\right)\qquad (10.14)$$
                $$m_{2}=\mu_{2}-\Lambda_{22}^{-1} \Lambda_{21}\left(\mathbb{E}\left[z_{1}\right]-\mu_{1}\right)\qquad (10.15)$$</p>
              </div>
              <div class="column">
                <p>この時、これらの解には相互依存関係があり、$q_{1}^{\star}(z_{1})$ は $q_{2}^{\star}(z_{2})$ を使って計算される期待値に依存し、その逆も成り立っています。</p>
                <p>したがって、一般に変分ベイズ法の解を求めるには、これを再推定を行う方程式と見て、ある収束条件が満たされるまで変数の値を順番に更新していくことになります。</p>
                <p>ただし、この問題は十分に簡単なため閉形式の解が求まり、$\mathbb{E}[z_{1}] = \mu_1$, $\mathbb{E}[z_{2}] = \mu_2$ となるときに式が成立し、分布が人喰いであるときにはこれが唯一の会であることは容易にわかります。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <p>この結果を下図に示します。ここで、今回行ったのはカルバックライブラーダイバージェンス $\mathrm{KL}(q \| p)$ の最小化であり、
                $$\mathrm{KL}(q||p) = - \int q(\mathbf{z})\ln \frac{p(\mathbf{z})}{q(\mathbf{z})}$$
                より、$q(\mathbf{z})$ の分散は $p(\mathbf{z})$ の分散が最小になる方向によって制御されており、それと直交する方向の分散は大きく過小評価されている事がわかります。</p>
                <p>これによって、分解による変分近似は事後分布をコンパクトに近似しすぎる傾向があります。</p>
              </div>
              <div class="column">
                <figure>
                  <img src="prml_static/images/Chap10/approximation problem.png" alt="approximation problem">
                </figure>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>EP法</h2>
                <p class="text-intro">導入</p>
                <p>比較のために、逆のかるバックライバーダイバージェンス $\mathrm{KL}(p||q)$ を最小化する場合を考えます。</p>
                <p>この形式のカルバックライブラーダイバージェンスはもう一つの近似推論の方法であるEP法で使われています。結果は、先ほどの結果と逆になります。</p>
              </div>
              <div class="column">
                <figure>
                  <img src="prml_static/images/Chap10/approximation problem2.png" alt="approximation problem2">
                </figure>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p class="text-intro">Variational Mixture of Gaussians</p>
                <p>ここで、ガウス混合モデルについて、変分推論法を適用してみます。</p>
                <p>この問題は、以下のグラフィカルモデルで示されたガウス混合モデルの尤度関数を求める事が出発点となります。</p>
                <figure>
                  <img src="prml_static/images/Chap10/Mixed Gaussian model.png" alt="Mixed Gaussian model">
                </figure>
                <p>各観測値 $\mathbf{x}_n$ には対応する潜在変数 $\mathbf{z}_n$ があり、これは $K$ 個の要素 $z_{nk}\quad(k=1,\ldots,K)$ の中に $1$ が一つだけある二値ベクトルです。</p>
              </div>
              <div class="column">
                <p>ここで、観測データの集合を $\mathbf{X} = \{\mathbf{x}_1,\ldots,\mathbf{x}_N\}$、対応する潜在変数を $\mathbf{Z} = \{\mathbf{z}_1,\ldots,\mathbf{z}_N\}$ と書くと、混合比 $\mathbf{\pi}$ が与えられた時の $\mathbf{Z}$ の条件付き分布は次の形式で書けます。
                $$p(\mathbf{Z} | \boldsymbol{\pi})=\prod_{n=1}^{N} \prod_{k=1}^{K} \pi_{k}^{z_{n k}}\qquad (10.37)$$</p>
                <p>同じように、潜在変数と混合要素のパラメータが与えられた時の観測データベクトルの条件付き分布は
                $$p(\mathbf{X} | \mathbf{Z}, \boldsymbol{\mu}, \boldsymbol{\Lambda})=\prod_{n=1}^{N} \prod_{k=1}^{K} \mathcal{N}\left(\mathbf{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}^{-1}\right)^{z_{n k}}\qquad (10.38)$$
                となります。なお、ここで $\boldsymbol{\mu} = \{\boldsymbol{\mu}_k\}$, $\boldsymbol{\Lambda} = \{\boldsymbol{\Lambda}_k\}$ です。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p class="text-intro">Variational Mixture of Gaussians</p>
                <p>次に、パラメータ $\boldsymbol{\mu},\boldsymbol{\Lambda},\boldsymbol{\pi}$ の事前分布を導入します。この時、計算が楽になるので、共役事前分布を使います。</p>
                <p class="text-intro">混合比 $\boldsymbol{\pi}$</p>
                <p>混合比 $\boldsymbol{\pi}$ の事前分布にはディリクレ分布を用います。
                $$p(\boldsymbol{\pi})=\operatorname{Dir}(\boldsymbol{\pi} | \boldsymbol{\alpha}_{0})=C\left(\boldsymbol{\alpha}_{0}\right) \prod_{k=1}^{K} \pi_{k}^{\alpha_{0}-1}\qquad (10.39)$$
                ここでは、要素の対称性を考えて、各混合要素について同じパタメータ $\boldsymbol{\alpha}_{0}$ を用いました。$C(\boldsymbol{\alpha}_{0})$ は、ディリクレ分布の正規化定数です。なお、パラメータ $\alpha_0$ は、各混合要素に対する実質的な事前の観測回数に相当すると解釈できます。</p>
              </div>
              <div class="column">
                <p class="text-intro">混合要素の持つガウス分布</p>
                <p>混合要素の持つガウス分布については、その平均と制度に次で与えられる独立なガウス-ウィシャート事前分布を導入します。
                $$\begin{aligned}
                p(\boldsymbol{\mu}, \boldsymbol{\Lambda})
                & =p(\boldsymbol{\mu} | \boldsymbol{\Lambda}) p(\boldsymbol{\Lambda}) \\
                & =\prod_{k=1}^{K} \mathcal{N}\left(\boldsymbol{\mu}_{k} | \mathbf{m}_{0},\left(\beta_{0} \mathbf{\Lambda}_{k}\right)^{-1}\right) \mathcal{W}\left(\boldsymbol{\Lambda}_{k} | \mathbf{W}_{0}, \nu_{0}\right)\qquad (10.40)
                \end{aligned}$$</p>
                <p>これは、平均も分散も未知の場合の共役事前分布を表しているからです。対称性から、通常 $ \mathbf{m}_{0} = \mathbf{0}$ と置きます。</p>
                <figure>
                  <img src="prml_static/images/Chap10/Mixed Gaussian model.png" alt="Mixed Gaussian model">
                </figure>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p class="text-intro">Variational Mixture of Gaussians</p>
                <p>このモデルを変分ベイズ法で扱うためには、次に全ての確率変数の同時分布を次のように書き下す必要があります。
                $$p(\mathbf{X}, \mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda}) = p(\mathbf{X} | \mathbf{Z}, \boldsymbol{\mu}, \boldsymbol{\Lambda}) p(\mathbf{Z} | \boldsymbol{\pi}) p(\boldsymbol{\pi}) p(\boldsymbol{\mu} | \boldsymbol{\Lambda}) p(\boldsymbol{\Lambda})\qquad (10.41)$$</p>
                <p>なお、観測されているのは $\mathbf{X}$ のみです。</p>
                <p>ここで、先のグラフィカルモデルのプレート内にある $\mathbf{z}_n$ のような変数は、数がデータ集合のサイズに比例して増えるため、<font color="red"><b>潜在変数</b></font>であり</p>
                <p>外側にある $\boldsymbol{\mu}$ のような変数はデータ集合の数とは独立に決まっているので、<font color="red"><b>パラメータ</b></font>と見なせます。</p>
              </div>
              <div class="column">
                <p>したがって、潜在変数とパラメータを分解した変分近似
                $$q(\mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda})=q(\mathbf{Z}) q(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda})\qquad (10.41)$$
                を考えることは理にかなっていると思えます。</p>
                <p>なお、この仮定がベイズ混合モデルについて実際に計算可能な解を得る上で必要な唯一の仮定です。</p>
                <p>ちなみに、(10.41)の分布 $p$、(10.42)の分布 $q$ の添字は省略されており、引数の違いによって異なる分布を表していることには注意が必要です。</p>
                <p>それでは、$q(\mathbf{Z}),q(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})$ の更新式を
                $$\ln q_{j}^{\star}(\mathbf{Z}_{j}) = \mathbb{E}_{i \neq j}[\ln p(\mathbf{X}, \mathbf{Z})]+\text { const. }\qquad (10.9)$$
                を用いることで、求めます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p class="text-intro">$q(\mathbf{Z})$ の更新式</p>
                <p>(10.9)を用いると、
                $$\begin{aligned}
                \ln q^{\star}(\mathbf{Z})
                & =\mathbb{E}_{\pi, \boldsymbol{\mu}, \boldsymbol{\Lambda}}[\ln p(\mathbf{X}, \mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda})]+\mathrm{const.}
                & (10.43)\\
                & =\mathbb{E}_{\pi, \boldsymbol{\mu}, \boldsymbol{\Lambda}}[\ln p(\mathbf{X} | \mathbf{Z}, \boldsymbol{\mu}, \boldsymbol{\Lambda}) + \ln p(\mathbf{Z} | \boldsymbol{\pi}) + \ln p(\boldsymbol{\pi}) +\ln p(\boldsymbol{\mu} | \boldsymbol{\Lambda}) +\ln p(\boldsymbol{\Lambda})]+\mathrm{const.}
                & \because (10.41)\\
                & =\mathbb{E}_{\pi}[\ln p(\mathbf{Z} | \boldsymbol{\pi})]+\mathbb{E}_{\boldsymbol{\mu}, \boldsymbol{\Lambda}}[\ln p(\mathbf{X} | \mathbf{Z}, \boldsymbol{\mu}, \boldsymbol{\Lambda})]+\mathrm{const.}
                & (10.44)\\
                & =\mathbb{E}_{\pi}\left[\ln \left(\prod_{n=1}^{N} \prod_{k=1}^{K} \pi_{k}^{z_{n k}}\right)\right]+\mathbb{E}_{\mu \Lambda}\left[\ln \left(\prod_{n=1}^{N} \prod_{k=1}^{K} N\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}^{-1}\right)^{z_{n k}}\right)\right]+\mathrm{const.}\\
                & =\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k} \mathbb{E}_{\pi_k}\left[\ln \pi_{k}\right]\\
                & +\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k} \mathbb{E}_{\mu_k \Lambda_k}\left[\frac{1}{2} \ln \left|\Lambda_{k}\right|-\frac{D}{2} \ln (2 \pi)-\frac{1}{2}\left(x_{n}-\mu_{k}\right)^{T} \Lambda_{k}\left(x_{n}-\mu_{k}\right)\right]+\mathrm{const.}
                \end{aligned}$$
                となります。$\mathbf{Z}$ に依存しない項は全て正規化定数に含まれてしまうので、$\mathbf{Z}$ への依存関係だけに興味があることに注意してください。
                </p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p class="text-intro">$q(\mathbf{Z})$ の更新式</p>
                <p>表記を楽にするために、先の結果を次のように書き換えます。
                $$\ln q^{\star}(\mathbf{Z})=\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k} \ln \rho_{n k}+\mathrm{const.}\qquad (10.45)$$
                $$\begin{aligned}
                \ln \rho_{n k}
                & = \mathbb{E}\left[\ln \pi_{k}\right]+\frac{1}{2} \mathbb{E}\left[\ln \left|\mathbf{\Lambda}_{k}\right|\right]-\frac{D}{2} \ln (2 \pi) \\
                & -\frac{1}{2} \mathbb{E}_{\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}}\left[\left(\mathbf{x}_{n} \mathbf{h} \boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{k}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)\right]\qquad (10.46)
                \end{aligned}$$</p>
                <p>ここで、(10.45)の両辺の指数を取ると、次式を得ます。
                $$q^{\star}(\mathbf{Z}) \propto \prod_{n=1}^{N} \prod_{k=1}^{K} \rho_{n k}^{z_{n k}}\qquad (10.48)$$</p>
              </div>
              <div class="column">
                <p>この分布のは正規化されている必要があるので、正規化定数を求めると、
                $$A = \prod_{n=1}^N\left(\sum_{k=1}^K\rho_{n k}\right)$$
                となるので、
                $$\begin{aligned}
                q^{\star}(\mathbf{Z})
                & = \frac{\prod_{n=1}^N\prod_{k=1}^K\rho_{nk}^{z_{nk}}}{\prod_{n=1}^N\left(\sum_{j=1}^K\rho_{n j}\right)}\\
                & = \frac{\left(\prod_{k=1}^K\rho_{1K}^{z_{1K}}\right)\cdots\left(\prod_{k=1}^K\rho_{NK}^{z_{NK}}\right)}{\left(\sum_{j=1}^K\rho_{1j}\right)\cdots\left(\sum_{j=1}^K\rho_{Nj}\right)}\\
                & = \prod_{n=1}^N\frac{\prod_{k=1}^K\rho_{nk}^{z_{nk}}}{\sum_{j=1}^K\rho_{nj}}\\
                & = \prod_{n=1}^N\prod_{k=1}^K\left(\frac{\rho_{nk}}{\sum_{j=1}^K\rho_{nj}}\right)^{z_{nk}}\\
                & = \prod_{n=1}^N\prod_{k=1}^Kr_{nk}^{z_{nk}}\qquad (10.48)(10.49)
                \end{aligned}$$</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p class="text-intro">正規化定数 $A$</p>
                <p>$$\begin{aligned}
                   A
                   & = \sum_{\mathbf{Z}}\prod_{n=1}^N\prod_{k=1}^K\rho_{n k}^{z_{n k}}\\
                   & = \left(\sum_{\mathbf{Z}_1}\prod_{k=1}^K\rho_{1 k}^{z_{1 k}}\right)\cdots\left(\sum_{\mathbf{Z}_N}\prod_{k=1}^K\rho_{N k}^{z_{N k}}\right)
                   & (\star_1)\\
                   & = \left(\sum_{k=1}^K\rho_{1 K}\right)\cdots\left(\sum_{k=1}^K\rho_{N K}\right)
                   & (\star_2)\\
                   & = \prod_{n=1}^N\left(\sum_{k=1}^K\rho_{n k}\right)
                   \end{aligned}$$</p>
              </div>
              <div class="column">
                <p>なお、$(\star_1)$ は以下の変形を行いました。
                $$\begin{aligned}
                \sum_{\mathbf{Z_1,Z_2}}f(\mathbf{Z_1})g(\mathbf{Z}_2)
                & = \sum_{\mathbf{Z}_2}\left\{f(\mathbf{Z}_1 = \{1,0\})g(\mathbf{Z}_2) + f(\mathbf{Z}_1 = \{0,1\})g(\mathbf{Z}_2)\right\}\\
                & = \left\{ f(\mathbf{Z}_1 = \{1,0\}) + f(\mathbf{Z}_1 = \{0,1\})\right\}\sum_{\mathbf{Z}_2}g(\mathbf{Z}_2)\\
                & = \sum_{\mathbf{Z}_1}f(\mathbf{Z}_1)\sum_{\mathbf{Z}_2}g(\mathbf{Z}_2)
                \end{aligned}$$</p>
                <p>また、$(\star_2)$ は以下の変形を行いました。
                $$\begin{aligned}
                \sum_{\mathbf{Z}_1}\prod_{k=1}^K\rho_{1 k}^{z_{1k}}
                & = \sum_{\mathbf{Z}_1}\rho_{11}^{z_{11}}\cdots\rho_{N1}^{z_{N1}}\\
                & = \left(\rho_{11}^1\rho_{12}^0\cdots\rho_{1K}^0\right) +
                    \left(\rho_{11}^0\rho_{12}^1\cdots\rho_{1K}^0\right) + \cdots +
                    \left(\rho_{11}^0\rho_{12}^0\cdots\rho_{1K}^1\right)\\
                & = \rho_{11} + \rho_{12} \cdots + \rho_{1K}\\
                & = \sum_{k=1}^K\rho_{1k}
                \end{aligned}$$</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p>ここで、因子 $q(\mathbf{Z})$ の最適解 $q^{\star}(\mathbf{Z})$ が、
                $$q^{\star}(\mathbf{Z}) = \prod_{n=1}^N\prod_{k=1}^Kr_{nk}^{z_{nk}}\qquad (10.48)$$であり、事前分布 $p(\mathbf{Z}|\boldsymbol{\mu})$ が
                $$p(\mathbf{Z} | \boldsymbol{\pi})=\prod_{n=1}^{N} \prod_{k=1}^{K} \pi_{k}^{z_{n k}}\qquad (10.37)$$と同じ形をしていることがわかります。</p>
                <p>ここで、離散分布 $q^{\star}(\mathbf{Z})$ については標準的な公式から
                $$\mathbb{E}[z_{n k}]=r_{n k}\qquad (10.50)$$
              　となるので、量 $r_{n k}$ は負担率を表していることがわかります。</p>
              </div>
              <div class="column">
                <p>$q^{\star}(\mathbf{Z})$ は、他の変数の分布によるモーメントに依存することに注意してください。これより、変分ベイズ法の更新式は相互関係にあり、繰り返しで解く必要があります。</p>
                <p>いま、観測データ集合について以下で与えられる、負担率から計算できる三つの統計量を定義しておくと便利です。
                $$\begin{aligned}
                N_{k}
                & =\sum_{n=1}^{N} r_{n k}
                & (10.51)\\
                \overline{\mathbf{x}}_{k}
                & =\frac{1}{N_{k}} \sum_{n=1}^{N} r_{n k} \mathbf{x}_{n}
                $ (10.52)\\
                \mathbf{S}_{k}
                & =\frac{1}{N_{k}} \sum_{n=1}^{N} r_{n k}\left(\mathbf{x}_{n}-\overline{\mathbf{x}}_{k}\right)\left(\mathbf{x}_{n}-\overline{\mathbf{x}}_{k}\right)^{\mathrm{T}}
                & (10.53)
                \end{aligned}$$</p>
                <p>これらは、ガウス混合モデルの最尤推定でのEMアルゴリズムで計算される量と似ています。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p class="text-intro">変分事後分布の因子 $q(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})$ の更新式</p>
                <p>一般的な結果(10.9)を用いると、
                $$\begin{aligned}
                \ln q^{\star} (\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda})
                & =\ln p(\boldsymbol{\pi})+\sum_{k=1}^{K} \ln p\left(\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}\right)+\mathbb{E}_{\mathbf{Z}}[\ln p(\mathbf{Z} | \boldsymbol{\pi})] \\
                & +\sum_{k=1}^{K} \sum_{n=1}^{N} \mathbb{E}\left[z_{n k}\right] \ln \mathcal{N}\left(\mathbf{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}^{-1}\right)+\mathrm{const.} \qquad(10.54)
                \end{aligned}$$となります。</p>
                <p>この式の右辺は、$\boldsymbol{\pi}$ だけを含む項と $\boldsymbol{\mu}$ および $\boldsymbol{\Lambda}$ だけを含む項の和に分解されることがわかります。</p>
                <p>これは、変分事後分布 $q(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda})$ が $q(\boldsymbol{\pi})q(\boldsymbol{\mu}, \boldsymbol{\Lambda})$ と分解されることを意味します。</p>
              </div>
              <div class="column">
                <p>さらに、$\boldsymbol{\mu}$ および $\boldsymbol{\Lambda}$ を含む項は $\boldsymbol{\mu}_k$ と $\boldsymbol{\Lambda}_k$ を含む $k$ についての積からなり、次のように分解されます。
                $$q(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda})=q(\boldsymbol{\pi}) \prod_{k=1}^{K} q\left(\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}\right)\qquad (10.55)$$</p>
                <p>ここで、(10.54)の右辺において $\boldsymbol{\pi}$ に依存する項を探すと、次を得ます。
                $$\begin{aligned}
                \ln q^{\star}(\boldsymbol{\pi})
                & = \ln p(\boldsymbol{\pi}) + \mathbb{E}_{\mathbf{Z}}[\ln p(\mathbf{Z} | \boldsymbol{\pi})] + \mathrm{const.}\\
                & = \left(\alpha_{0}-1\right) \sum_{k=1}^{K} \ln \pi_{k}+\sum_{k=1}^{K} \sum_{n=1}^{N} r_{n k} \ln \pi_{k}+\mathrm{const.}\qquad (10.56)
                \end{aligned}$$</p>
                <p>この計算には、(10.50)を用いました。(10.56)の両辺の指数を取ると、$q^{\star}(\boldsymbol{\pi})$ はディリクレ分布
                $$q^{\star}(\boldsymbol{\pi})=\operatorname{Dir}(\boldsymbol{\pi} | \boldsymbol{\alpha})\qquad (10.57)$$
              　となることがわかります。なお、$\boldsymbol{\alpha}$ の要素 $\alpha_k$ は次式です。
                $$\alpha_{k}=\alpha_{0}+N_{k}\qquad (10.58)$$</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p class="text-intro">変分事後分布 $q^{\star}(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)$ の更新式</p>
                <p>変分事後分布 $q^{\star}(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)$ は周辺分布の積には分解できませんが、乗法定理は必ず適用できるので、$q^{\star}\left(\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}\right)=q^{\star}\left(\boldsymbol{\mu}_{k} | \boldsymbol{\Lambda}_{k}\right) q^{\star}\left(\mathbf{\Lambda}_{k}\right)$ の形に書くことができます。</p>
                <p>(10.54)の中で $\boldsymbol{\mu}_k$ および $\boldsymbol{\Lambda}_k$ を含む項を調べると、
                $$\begin{aligned}
                \ln q^{\star}(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)
                & = \ln p\left(\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}\right) + \sum_{n=1}^{N} \mathbb{E}\left[z_{n k}\right] \ln \mathcal{N}\left(\mathbf{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}^{-1}\right)+\mathrm{const.}\\
                & = \ln \mathcal{N}\left(\boldsymbol{\mu}_{k} | \mathbf{m}_{0},\left(\beta_{0} \mathbf{\Lambda}_{k}\right)^{-1}\right) \mathcal{W}\left(\boldsymbol{\Lambda}_{k} | \mathbf{W}_{0}, \nu_{0}\right) + \sum_{n=1}^N r_{n k}\ln \mathcal{N}\left(\mathbf{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}^{-1}\right)+\mathrm{const.}\\
                & = \frac{1}{2}\ln |\boldsymbol{\Lambda}_k| - \color{red}{\frac{1}{2}(\boldsymbol{\mu}_k - \mathbf{m}_{0})^T\beta_{0} \mathbf{\Lambda}_{k}(\boldsymbol{\mu}_k - \mathbf{m}_{0})} + \frac{\nu_{0} - D -1}{2}\ln |\boldsymbol{\Lambda}_k| - \frac{1}{2}\mathrm{Tr}(\mathbf{W}_{0}^{-1}\boldsymbol{\Lambda}_k)\\
                & + \sum_{n=1}^N r_{n k}\left\{\frac{1}{2}\ln |\boldsymbol{\Lambda}_k| -\color{red}{\frac{1}{2}(\mathbf{x}_n-\boldsymbol{\mu}_k)^T\boldsymbol{\Lambda_k}(\mathbf{x}_n-\boldsymbol{\mu}_k)}\right\} + \mathrm{const.}
                \end{aligned}$$
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p class="text-intro">変分事後分布 $q^{\star}(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)$ の更新式</p>
                <p>赤い部分を整理して平方完成をすると、次の結果を得ます。
                $$q^{\star}\left(\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}\right)=\mathcal{N}\left(\boldsymbol{\mu}_{k} | \mathbf{m}_{k},\left(\beta_{k} \boldsymbol{\Lambda}_{k}\right)^{-1}\right) \mathcal{W}\left(\boldsymbol{\Lambda}_{k} | \mathbf{W}_{k}, \nu_{k}\right)\qquad (10.59)$$</p>
                <p>ここで、以下のように変数を定義しました。
                $$\begin{aligned}
                \beta_{k}
                & =\beta_{0}+N_{k}
                & (10.60)\\
                \mathbf{m}_{k}
                & =\frac{1}{\beta_{k}}\left(\beta_{0} \mathbf{m}_{0}+N_{k} \overline{\mathbf{x}}_{k}\right)
                & (10.61)\\
                \mathbf{W}_{k}^{-1}
                & =\mathbf{W}_{0}^{-1}+N_{k} \mathbf{S}_{k}+\frac{\beta_{0} N_{k}}{\beta_{0}+N_{k}}\left(\overline{\mathbf{x}}_{k}-\mathbf{m}_{0}\right)\left(\overline{\mathbf{x}}_{k}-\mathbf{m}_{0}\right)^{\mathrm{T}}
                & (10.62)\\
                \nu_{k}
                & =\nu_{0}+N_{k}
                & (10.63)
                \end{aligned}$$</p>
              </div>
              <div class="column">
                <p>これらの更新式は、混合ガウス分布を最尤推定で解くEMアルゴリズムのMステップに似ています。</p>
                <p>モデルのパラメータの変分事後分布を更新するために必要な計算には、データ集合について最尤推定の場合に現れるものと同様の和を求めることになることがわかります。</p>
                <p>この変分Mステップを実行するためには、負担率を表す期待値 $\mathbb{E}\left[z_{n k}\right] = r_{n k}$ が必要になりますが、これは
                $$\begin{aligned}
                \ln \rho_{n k}
                & = \mathbb{E}\left[\ln \pi_{k}\right]+\frac{1}{2} \mathbb{E}\left[\ln \left|\mathbf{\Lambda}_{k}\right|\right]-\frac{D}{2} \ln (2 \pi) \\
                & -\frac{1}{2} \mathbb{E}_{\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}}\left[\left(\mathbf{x}_{n} \mathbf{h} \boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{k}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)\right]\qquad (10.46)
                \end{aligned}$$
                を正規化すれば
                $$r_{n k} = \frac{\rho_{n k}}{\sum_{j=1}^K\rho_{n j}}\qquad (10.49)$$から得られます。</p>
                <p></p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p class="text-intro">期待値 $\mathbb{E}\left[z_{n k}\right] = r_{n k}$</p>
                <p>$q^{\star}\left(\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}\right)=q^{\star}\left(\boldsymbol{\mu}_{k} | \boldsymbol{\Lambda}_{k}\right) q^{\star}\left(\mathbf{\Lambda}_{k}\right)$ の形に分解できることを利用すると、
                $$\begin{aligned}
                q^{\star}(\boldsymbol{\mu}_k|\boldsymbol{\Lambda}_k)
                & = \mathcal{N}\left(\boldsymbol{\mu}_k|\mathbf{m}_k,\left(\beta_k\boldsymbol{\Lambda}_k\right)^{-1}\right)\\
                q^{\star}(\boldsymbol{\Lambda}_k)
                & = \mathcal{W}\left(\boldsymbol{\Lambda}_{k} | \mathbf{W}_{k}, \nu_{k}\right)
                \end{aligned}$$と書けることがわかります。</p>
                <p>この時、以下のように期待値を計算すると、
                $$\begin{aligned}
                \mathbb{E}_{\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}}\left[\boldsymbol{\Lambda_k}\right]
                & = \int q^{\star}(\boldsymbol{\mu}_k|\boldsymbol{\Lambda}_k)q^{\star}(\boldsymbol{\Lambda}_k)\boldsymbol{\Lambda}_kd\boldsymbol{\mu}_kd\boldsymbol{\Lambda}_k\\
                & = \int\left[\int q^{\star}(\boldsymbol{\mu}_k|\boldsymbol{\Lambda}_k)d\boldsymbol{\mu}_k\right]q^{\star}(\boldsymbol{\Lambda}_k)\boldsymbol{\Lambda}_kd\boldsymbol{\Lambda}_k\\
                & = \int q^{\star}(\boldsymbol{\Lambda}_k)\boldsymbol{\Lambda}_kd\boldsymbol{\Lambda}_k\\
                & = \nu_{k}\mathbf{W}_{k}
                \end{aligned}$$</p>
              </div>
              <div class="column">
                <p>同様にして、
                $$\begin{aligned}
                \mathbb{E}_{\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}}\left[\boldsymbol{\mu}_k^T\boldsymbol{\Lambda_k}\right]
                & = \mathbf{m}_k^T\nu_{k}\mathbf{W}_{k}\\
                \mathbb{E}_{\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}}\left[\boldsymbol{\Lambda_k}\boldsymbol{\mu}_k\right]
                & = \nu_{k}\mathbf{W}_{k}\mathbf{m}_k\\
                \mathbb{E}_{\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}}\left[\boldsymbol{\mu}_k^T\boldsymbol{\Lambda_k}\boldsymbol{\mu}_k\right]
                & = \beta_k^{-1}D + \mathbf{m}_k^T\nu_{k}\mathbf{W}_{k}\mathbf{m}
                \end{aligned}$$と求まるので、
                $$\begin{aligned}
                \mathbb{E}_{\mu_{k}, \Lambda_{k}}\left[\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{k}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)\right]
                & = D \beta_{k}^{-1}+\nu_{k}\left(\mathbf{x}_{n}-\mathbf{m}_{k}\right)^{\mathrm{T}} \mathbf{W}_{k}\left(\mathbf{x}_{n}-\mathbf{m}_{k}\right) & (10.64)\\
                \ln \tilde{\Lambda}_{k} \equiv \mathbb{E}\left[\ln \left|\boldsymbol{\Lambda}_{k}\right|\right]
                & =\sum_{i=1}^{D} \psi\left(\frac{\nu_{k}+1-i}{2}\right)+D \ln 2+\ln \left|\mathbf{W}_{k}\right| & (10.65) \\
                \ln \widetilde{\pi}_{k} \equiv \mathbb{E}\left[\ln \pi_{k}\right]
                & =\psi\left(\alpha_{k}\right)-\psi(\widehat{\alpha}) & (10.66)
                \end{aligned}$$
                </p>
                <p>ここで、$\tilde{\Lambda}_{k}$ および $\widetilde{\pi}_{k}$ を新しく導入しました。また、$\psi(\cdot)$ はディガンマ関数、$\widehat{\alpha}=\sum_{k} \alpha_{k}$ です。(10.65)および(10.66)は、ウィシャート分布とディリクレ分布の標準的な性質から得られます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>変分混合ガウス分布</h2>
                <p class="text-intro">期待値 $\mathbb{E}\left[z_{n k}\right] = r_{n k}$</p>
                <p>ここまでの結果を用いると、負担率について以下の結果を得ることができます。
                $$r_{n k} \propto \widetilde{\pi}_{k} \widetilde{\Lambda}_{k}^{1 / 2} \exp \left\{-\frac{D}{2 \beta_{k}}-\frac{\nu_{k}}{2}\left(\mathbf{x}_{n}-\mathbf{m}_{k}\right)^{\mathrm{T}} \mathbf{W}_{k}\left(\mathbf{x}_{n}-\mathbf{m}_{k}\right)\right\}\qquad (10.67)$$
                この結果は最尤推定でのEMアルゴリズムの場合に対応する結果
                $$r_{n k} \propto \pi_{k}\left|\mathbf{\Lambda}_{k}\right|^{1 / 2} \exp \left\{-\frac{1}{2}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{k}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)\right\}\qquad (10.68)$$
                と類似しています。</p>
                <p>以上より、変分事後分布を最適化するには、最尤推定のEMアルゴリズムと似たEステップとMステップを相互に繰り返すことになります。</p>
              </div>
              <div class="column">
                <p class="text-intro">まとめ</p>
                <p>変分Eステップでは、モデルパラメータの現在の事後分布を用いて
                $$\begin{aligned}
                \mathbb{E}_{\mu_{k}, \Lambda_{k}}\left[\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{k}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)\right]
                & = D \beta_{k}^{-1}+\nu_{k}\left(\mathbf{x}_{n}-\mathbf{m}_{k}\right)^{\mathrm{T}} \mathbf{W}_{k}\left(\mathbf{x}_{n}-\mathbf{m}_{k}\right) & (10.64)\\
                \ln \tilde{\Lambda}_{k} \equiv \mathbb{E}\left[\ln \left|\boldsymbol{\Lambda}_{k}\right|\right]
                & =\sum_{i=1}^{D} \psi\left(\frac{\nu_{k}+1-i}{2}\right)+D \ln 2+\ln \left|\mathbf{W}_{k}\right| & (10.65) \\
                \ln \widetilde{\pi}_{k} \equiv \mathbb{E}\left[\ln \pi_{k}\right]
                & =\psi\left(\alpha_{k}\right)-\psi(\widehat{\alpha}) & (10.66)
                \end{aligned}$$
                のモーメントを計算し、負担率 $\mathbb{E}\left[z_{n k}\right] = r_{n k}$ を求めます。</p>
                <p>変分Mステップでは、それらの負担率を固定し、
                $$\begin{aligned}
                q^{\star}(\boldsymbol{\pi})
                & =\operatorname{Dir}(\boldsymbol{\pi} | \boldsymbol{\alpha})
                & (10.57)\\
                q^{\star}\left(\boldsymbol{\mu}_{k}, \boldsymbol{\Lambda}_{k}\right)
                & =\mathcal{N}\left(\boldsymbol{\mu}_{k} | \mathbf{m}_{k},\left(\beta_{k} \boldsymbol{\Lambda}_{k}\right)^{-1}\right) \mathcal{W}\left(\boldsymbol{\Lambda}_{k} | \mathbf{W}_{k}, \nu_{k}\right)
                & (10.59)
                \end{aligned}$$からパラメータの事後分布を求めます。</p>
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2>EP法</h2>
                <p class="text-intro">まだ読めてないンゴ</p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple">
          <div class="wrap">
            <div class="grid vertical-align">
              <div class="column">
                <h2></h2>
                <p class="text-intro"></p>
              </div>
              <div class="column">
              </div>
            </div>
          </div>
        </section>

        <section class="bg-apple aligncenter">
          <h2 class="text-emoji zoomIn">😊</h2>
          <h3><strong>Thank you!</strong></h2>
          <p><a href="https://twitter.com/cabernet_rock" title="@cabernet_rock on Twitter">@cabernet_rock</a></p>
        </section>

        <section class="bg-apple aligncenter">
          <!-- .wrap = container (width: 90%) -->
          <div class="wrap">
            <h2><strong>Please see my YouTube </strong></h2>
            <p class="text-intro">I'm explaining this slide.</p>
            <p>
              <a href="#" class="button" title="See YouTube">
                <svg class="fa-youtube">
                  <use xlink:href="#fa-youtube"></use>
                </svg>
                See my YouTube
              </a>
            </p>
          </div>
        </section>

      </article>
    </main>
    <!--main-->

    <!-- Required -->
    <script src="prml_static/js/webslides.js"></script>

    <script>
      window.ws = new WebSlides();
    </script>

    <!-- OPTIONAL - svg-icons.js (fontastic.me - Font Awesome as svg icons) -->
    <script defer src="prml_static/js/svg-icons.js"></script>

  </body>
</html>
